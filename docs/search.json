[
  {
    "objectID": "mp01.html#exploratory-analysis-data",
    "href": "mp01.html#exploratory-analysis-data",
    "title": "Mini-Project #01: Gourmet Cheeseburgers Across the Globe",
    "section": "Exploratory Analysis Data",
    "text": "Exploratory Analysis Data\n1) How many different countries does Netflix operate in? (You can use the viewing history as a proxy for countries in which Netflix operates.)\n\n\nCode\nCountries_total &lt;- COUNTRY_TOP_10 |&gt; \n  distinct(country_name) |&gt; \n  count()\n\n\nNetflix operates in 94 countries.\n2) Which non-English-language film has spent the most cumulative weeks in the global top 10? How many weeks did it spend?\n\n\nCode\nne_mcmtv_wks &lt;- GLOBAL_TOP_10 |&gt;\n  filter(category == \"Films (Non-English)\") |&gt;\n  slice_max(cumulative_weeks_in_top_10, n=1)\n\nnon_english_title &lt;- ne_mcmtv_wks$show_title\nnon_english_weeks &lt;- ne_mcmtv_wks$cumulative_weeks_in_top_10\n\n\nThe non-English movie that has spent the most cumulative weeks in the global Top 10 is All Quiet on the Western Front for a total of 23 weeks.\n3) What is the longest film (English or non-English) to have ever appeared in the Netflix global Top 10? How long is it in minutes?\n\n\nCode\nlongest_film &lt;- GLOBAL_TOP_10 |&gt; \n  filter(str_detect(category, \"Films\")) |&gt;\n  filter(!is.na(runtime)) |&gt;\n  mutate(runtime_minutes = round(60 * runtime)) |&gt;\n  arrange(desc(runtime_minutes)) |&gt;\n  slice(1)\n\nlongest_film_name &lt;- longest_film$show_title\nlongest_film_mins &lt;- longest_film$runtime_minutes\n\n\nThe longest film to ever appear the longest in global Top 10 is Pushpa 2: The Rule (Reloaded Version) for 224 minutes.\n4) For each of the four categories, what program has the most total hours of global viewership?\n\n\nShow Code\nmost_hours_prgms &lt;- GLOBAL_TOP_10 |&gt;\n  group_by(category, show_title) |&gt; \n  summarise(total_hours = sum(weekly_hours_viewed)) |&gt;\n  group_by(category) |&gt; \n  slice_max(total_hours, n=1) |&gt;\n  ungroup() |&gt;\n  arrange(desc(total_hours))\n\n\n\n\nCode\nmost_hours_prgms |&gt;\n  mutate(total_hours = scales::comma(total_hours)) |&gt;\n  format_titles() |&gt;\n  datatable(\n    most_hours_prgms, \n    options = list(searching = FALSE, info = FALSE),\n    caption = \"Top Programs' Total Hours by Category\")\n\n\n\n\n\n\n5) Which TV show had the longest run in a country’s Top 10? How long was this run and in what country did it occur?\n\n\nCode\nlongest_cn_run &lt;- COUNTRY_TOP_10 |&gt;\n  filter(str_detect(category, \"TV\")) |&gt;\n  group_by(country_name, show_title) |&gt;\n  summarise(mx_cmwks = max(cumulative_weeks_in_top_10, na.rm = TRUE), .groups = \"drop\") |&gt;\n  arrange(desc(mx_cmwks)) |&gt;\n  slice(1)\n\nlongest_cn_run_title &lt;- longest_cn_run$show_title\nlongest_cn_run_weeks &lt;- longest_cn_run$mx_cmwks\nlongest_cn_run_name &lt;- longest_cn_run$country_name\n\n\nMoney Heist had the longest run in a Pakistan’s Top 10 for a total of 127 weeks.\n6) Netflix provides over 200 weeks of service history for all but one country in our data set. Which country is this and when did Netflix cease operations in that country?\n\n\nCode\nna_200 &lt;- COUNTRY_TOP_10 |&gt;\n  group_by(country_name) |&gt;\n  summarise(total_least_weeks = n_distinct(week), last_wk = max(week, na.rm = TRUE), .groups = \"drop\") |&gt;\n  filter(total_least_weeks &lt; 200) |&gt;\n  arrange(total_least_weeks)\n\nleast_weeks_country &lt;- na_200$country_name\nleast_weeks_date &lt;- na_200$last_wk\n\n\nOn 2022-02-27, Netflix ceased their operations in Russia.\n7) What is the total viewership of the TV show Squid Game? Note that there are three seasons total and we are looking for the total number of hours watched across all seasons.\n\n\nCode\ntotal_viewership_sg &lt;- GLOBAL_TOP_10 |&gt;\n  filter(str_detect(show_title, \"Squid Game\")) |&gt;\n  summarise(total_hours_viewed = sum(weekly_hours_viewed, na.rm = TRUE)) |&gt;\n  pull(total_hours_viewed)\ntotal_viewership_squid_game &lt;- scales::comma(total_viewership_sg)\n\n\nThe total viewership of the TV show Squid Game is 5,310,000,000 hours, taking into account all three seasons.\n8) The movie Red Notice has a runtime of 1 hour and 58 minutes. Approximately how many views did it receive in 2021? Note that Netflix does not provide the weekly_views values that far back in the past, but you can compute it yourself using the total view time and the runtime.\n\n\nCode\nlibrary(lubridate)\nRed_Notice_21 &lt;- GLOBAL_TOP_10 |&gt;\n  filter(show_title == \"Red Notice\", year(week) == 2021) |&gt;\n  summarise(total_hours = sum(weekly_hours_viewed, na.rm = TRUE)) |&gt;\n  mutate(approx_views = total_hours / (1 + 58/60))\n\n\nThe total approximate views Red Notice received in 2021 is 201,732,203 views.\n9) How many Films reached Number 1 in the US but did not originally debut there? That is, find films that first appeared on the Top 10 chart at, e.g., Number 4 but then became more popular and eventually hit Number 1? What is the most recent film to pull this off?\n\n\nCode\ntop_US_films &lt;- COUNTRY_TOP_10 |&gt;\n  filter(country_name == \"United States\", str_detect(category, \"Films\")) |&gt;\n  arrange(week) |&gt;\n  group_by(show_title) |&gt;\n  summarise(\n    debut_week = min(week, na.rm = TRUE),\n    debut_rank = first(weekly_rank),\n    top_rank = min(weekly_rank, na.rm = TRUE),\n    latest_week = max(week)\n    ) |&gt;\n  filter(debut_rank &gt; 1, top_rank == 1)\n\nlastest_film &lt;- top_US_films |&gt; \n  arrange(desc(latest_week)) |&gt;\n  slice(1) \n\ntop_US_films |&gt;\n  format_titles() |&gt;\n  datatable(\n    options = list(searching= FALSE, info= FALSE), \n    caption = \"Films That Climed to Number 1\"\n  )\n\n\n\n\n\n\nCode\ntotal_top_films &lt;- nrow(top_US_films)\nrecent_top_1 &lt;- lastest_film$show_title\n\n\nThere is a total of 45 films that did not originally debut at number 1, but eventually ranked at the top. The most recent film to achieve this was KPop Demon Hunters.\n10) Which TV show/season hit the top 10 in the most countries in its debut week? In how many countries did it chart?\n\n\nCode\ntop_show_debut &lt;- COUNTRY_TOP_10 |&gt;\n  filter(category == \"TV\") |&gt;\n  group_by(show_title) |&gt;\n  filter(week == min(week)) |&gt; \n  summarise(countries_amount = n_distinct(country_name), .groups = \"drop\") |&gt;\n  arrange(desc(countries_amount)) |&gt; \n  slice(1)\n\nshow_name &lt;- top_show_debut$show_title\ncountries_charted &lt;- top_show_debut$countries_amount\n\n\nEmily in Paris hit the Top 10 in most countries in its debut week, with a total of 94 countries charted."
  },
  {
    "objectID": "mp01.html#the-rise-of-the-upside-down",
    "href": "mp01.html#the-rise-of-the-upside-down",
    "title": "Mini-Project #01: Gourmet Cheeseburgers Across the Globe",
    "section": "The Rise of the Upside Down",
    "text": "The Rise of the Upside Down\n\nPress Release 1: Upcoming Season of Stranger Things\n\n\n\nStranger Things Season 5\n\n\n\n\nCode\nlibrary(lubridate)\nst_views &lt;- GLOBAL_TOP_10 |&gt;\n  filter(show_title == \"Stranger Things\") |&gt;\n  summarise(total_hours = sum(weekly_hours_viewed, na.rm = TRUE))\n\nst_data &lt;- GLOBAL_TOP_10 %&gt;%\n  filter(str_detect(show_title, regex(\"Stranger Things\", ignore_case = TRUE))) %&gt;%\n  summarise(\n    total_weeks = max(cumulative_weeks_in_top_10, na.rm = TRUE)\n  )\n\ncompare_shows &lt;- GLOBAL_TOP_10 |&gt;\n  select(show_title, weekly_hours_viewed) |&gt; \n  filter(show_title == \"Stranger Things\" | \n           show_title == \"My Life With the Walter Boys\" | \n           show_title ==\"Love Is Blind: UK\" | \n           show_title == \"You\") |&gt; \n  group_by(show_title) |&gt; \n  summarise(total_weekly_hours = sum(weekly_hours_viewed))\n\nwalter_boys &lt;- compare_shows$show_title[2]\nyou_show &lt;- compare_shows$show_title[4]\nwalter_views &lt;- compare_shows$total_weekly_hours[2]\nyou_views &lt;- compare_shows$total_weekly_hours[4]\n\n\nAt last, Stranger Things is coming to an end, releasing its final season on November 26, 2025. Not only has it been a huge hit across the United States, but across other nations as well, garnering a total viewership of 2,967,980,000 hours. Compared to other popular shows such as You and My Life With the Walter Boys, they fell behind with 1,542,990,000 and 629,800,000 hours, respectively. These are astounding numbers, as we saw it was one of the top viewed shows and also ranked highly for several weeks. It has maintained itself 19 weeks in the Top 10.\nAs a Stranger Things watcher myself, I cannot wait to see if Hawkins will be consumed by the underworld, or if it will finally be closed for good."
  },
  {
    "objectID": "mp01.html#breaking-into-india-netflixs-ambitions-in-the-indian-market",
    "href": "mp01.html#breaking-into-india-netflixs-ambitions-in-the-indian-market",
    "title": "Mini-Project #01: Gourmet Cheeseburgers Across the Globe",
    "section": "Breaking Into India: Netflix’s Ambitions in the Indian Market",
    "text": "Breaking Into India: Netflix’s Ambitions in the Indian Market\n\nPress Release 2: Commercial Success in India\n\n\nCode\ntop_hindi_films &lt;- GLOBAL_TOP_10 |&gt; select(show_title, weekly_hours_viewed) |&gt; \n  filter(str_detect(show_title, \"Hindi\")) |&gt; \n  group_by(show_title) |&gt; \n  summarise(total_wkly_hrs = sum(weekly_hours_viewed)) |&gt;\n  slice_max(total_wkly_hrs, n = 5)\n  \nhindi_india &lt;- COUNTRY_TOP_10 %&gt;%\n  filter(country_name == \"India\") %&gt;%\n  filter(str_detect(show_title, regex(\"Hindi\", ignore_case = TRUE)) |\n         str_detect(season_title, regex(\"Hindi\", ignore_case = TRUE))) %&gt;%\n  group_by(show_title) %&gt;%\n  summarise(\n    weeks_top10 = max(cumulative_weeks_in_top_10, na.rm = TRUE),\n    peak_rank   = min(weekly_rank, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %&gt;%\n  arrange(peak_rank, desc(weeks_top10))\n\nrrr &lt;- top_hindi_films$show_title[1]\ntop_hindi_2 &lt;- top_hindi_films$show_title[2]\nrrr_hours &lt;-top_hindi_films$total_wkly_hrs[1]\ntop2_hours &lt;- top_hindi_films$total_wkly_hrs[2]\nrrr_rank_weeks &lt;- hindi_india$weeks_top10[1]\n\n\nBeing the most populated country in the world, India is an enticing market many companies are ready to enter and invest in, Netlfix being one of them. Currently RRR (Hindi) and Kalki 2898 AD (Hindi) sit at the top with 79,780,000 and 23,100,000 hours, respectively. Not to mention RRR (Hindi) has spent 25 weeks ranked 1. A total of 11 films have ranked that high, attracting Netflix even more."
  },
  {
    "objectID": "mp01.html#global-hits-non-english-tv-shows-taking-over-worldwide",
    "href": "mp01.html#global-hits-non-english-tv-shows-taking-over-worldwide",
    "title": "Mini-Project #01: Gourmet Cheeseburgers Across the Globe",
    "section": "Global Hits: Non-English TV Shows Taking Over Worldwide",
    "text": "Global Hits: Non-English TV Shows Taking Over Worldwide\n\nPress Release 3: Open Topic\n\n\nCode\nne__shows &lt;- GLOBAL_TOP_10 |&gt;\n  filter(category == \"TV (Non-English)\") |&gt;\n  group_by(show_title) |&gt;\n  summarise(\n    total_weeks = max(cumulative_weeks_in_top_10, na.rm = TRUE),\n    total_hours = sum(weekly_hours_viewed, na.rm = TRUE),\n    .groups = \"drop\"\n  ) |&gt;\n  arrange(desc(total_weeks))\n\nne__shows |&gt;\n  mutate(total_hours = scales::comma(total_hours)) |&gt;\n  head(10) |&gt;\n  format_titles() |&gt;\n  datatable(\n    options = list(searching= FALSE, info= FALSE), \n    caption = \"Top Non-English Shows\"\n  )\n\n\n\n\n\n\nCode\nbetty_show &lt;- ne__shows$show_title[2]\nbetty_wks &lt;- ne__shows$total_weeks[2]\nbetty_hours&lt;- ne__shows$total_hours[2]\n\n\nOver the last decade, we have seen an influx of foreign consumption worldwide. The table above demonstrates the TV shows’ amount of weeks spent in the Top 10. Aside from genre, there is also a variety of language in just these five shows, like Korean, Spanish, and Portuguese. Squid Game did extremly well, but Yo soy Betty, la fea did exceptional as well, with 30 weeks in the Top 10 and 297,560,000 hours in viewership. These Korean dramas and Spanish thrillers continue breaking barriers and dominating global charts."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My First Website",
    "section": "",
    "text": "My name is Karen Cruz and I am currently 22 years old. I am currently on my second semester doing the Master’s in Statistics program at CUNY Baruch.\nThere’s a multitude of things I enjoy doing during my free time such as trying new foods and restaurants, playing sports, traveling, and the list goes on. Fortunately, I reside in NYC, where you can try gastronomy from all over the world at every corner of the city. As for traveling, one of my favorite places I have traveled to was Kyoto, Japan. It was such a surreal experience I think about everyday. This is my LinkenIn, if anyone wants to follow me!"
  },
  {
    "objectID": "index.html#nice-to-meet-everybody",
    "href": "index.html#nice-to-meet-everybody",
    "title": "My First Website",
    "section": "Nice to meet everybody!",
    "text": "Nice to meet everybody!"
  },
  {
    "objectID": "Untitled.html#exploratory-analysis-data",
    "href": "Untitled.html#exploratory-analysis-data",
    "title": "Mini-Project #01: Gourmet Cheeseburgers Across the Globe",
    "section": "Exploratory Analysis Data",
    "text": "Exploratory Analysis Data\n1) How many different countries does Netflix operate in? (You can use the viewing history as a proxy for countries in which Netflix operates.)\n\n\nCode\nCountries_total &lt;- COUNTRY_TOP_10 |&gt; \n  distinct(country_name) |&gt; \n  count()\n\n\nNetflix operates in 94 countries.\n2) Which non-English-language film has spent the most cumulative weeks in the global top 10? How many weeks did it spend?\n\n\nCode\nne_mcmtv_wks &lt;- GLOBAL_TOP_10 |&gt;\n  filter(category == \"Films (Non-English)\") |&gt;\n  slice_max(cumulative_weeks_in_top_10, n=1)\n\nnon_english_title &lt;- ne_mcmtv_wks$show_title\nnon_english_weeks &lt;- ne_mcmtv_wks$cumulative_weeks_in_top_10\n\n\nThe non-English movie that has spent the most cumulative weeks in the global Top 10 is All Quiet on the Western Front for a total of 23 weeks.\n3) What is the longest film (English or non-English) to have ever appeared in the Netflix global Top 10? How long is it in minutes?\n\n\nCode\nlongest_film &lt;- GLOBAL_TOP_10 |&gt; \n  filter(str_detect(category, \"Films\")) |&gt;\n  filter(!is.na(runtime)) |&gt;\n  mutate(runtime_minutes = round(60 * runtime)) |&gt;\n  arrange(desc(runtime_minutes)) |&gt;\n  slice(1)\n\nlongest_film_name &lt;- longest_film$show_title\nlongest_film_mins &lt;- longest_film$runtime_minutes\n\n\nThe longest film to ever appear the longest in global Top 10 is Pushpa 2: The Rule (Reloaded Version) for 224 minutes.\n4) For each of the four categories, what program has the most total hours of global viewership?\n\n\nShow Code\nmost_hours_prgms &lt;- GLOBAL_TOP_10 |&gt;\n  group_by(category, show_title) |&gt; \n  summarise(total_hours = sum(weekly_hours_viewed)) |&gt;\n  group_by(category) |&gt; \n  slice_max(total_hours, n=1) |&gt;\n  ungroup() |&gt;\n  arrange(desc(total_hours))\n\n\n\n\nCode\nmost_hours_prgms |&gt;\n  mutate(total_hours = scales::comma(total_hours)) |&gt;\n  format_titles() |&gt;\n  datatable(\n    most_hours_prgms, \n    options = list(searching = FALSE, info = FALSE),\n    caption = \"Top Programs' Total Hours by Category\")\n\n\n\n\n\n\n5) Which TV show had the longest run in a country’s Top 10? How long was this run and in what country did it occur?\n\n\nCode\nlongest_cn_run &lt;- COUNTRY_TOP_10 |&gt;\n  filter(str_detect(category, \"TV\")) |&gt;\n  group_by(country_name, show_title) |&gt;\n  summarise(mx_cmwks = max(cumulative_weeks_in_top_10, na.rm = TRUE), .groups = \"drop\") |&gt;\n  arrange(desc(mx_cmwks)) |&gt;\n  slice(1)\n\nlongest_cn_run_title &lt;- longest_cn_run$show_title\nlongest_cn_run_weeks &lt;- longest_cn_run$mx_cmwks\nlongest_cn_run_name &lt;- longest_cn_run$country_name\n\n\nMoney Heist had the longest run in a Pakistan’s Top 10 for a total of 127 weeks.\n6) Netflix provides over 200 weeks of service history for all but one country in our data set. Which country is this and when did Netflix cease operations in that country?\n\n\nCode\nna_200 &lt;- COUNTRY_TOP_10 |&gt;\n  group_by(country_name) |&gt;\n  summarise(total_least_weeks = n_distinct(week), last_wk = max(week, na.rm = TRUE), .groups = \"drop\") |&gt;\n  filter(total_least_weeks &lt; 200) |&gt;\n  arrange(total_least_weeks)\n\nleast_weeks_country &lt;- na_200$country_name\nleast_weeks_date &lt;- na_200$last_wk\n\n\nOn 2022-02-27, Netflix ceased their operations in Russia.\n7) What is the total viewership of the TV show Squid Game? Note that there are three seasons total and we are looking for the total number of hours watched across all seasons.\n\n\nCode\ntotal_viewership_sg &lt;- GLOBAL_TOP_10 |&gt;\n  filter(str_detect(show_title, \"Squid Game\")) |&gt;\n  summarise(total_hours_viewed = sum(weekly_hours_viewed, na.rm = TRUE)) |&gt;\n  pull(total_hours_viewed)\ntotal_viewership_squid_game &lt;- scales::comma(total_viewership_sg)\n\n\nThe total viewership of the TV show Squid Game is 5,310,000,000 hours, taking into account all three seasons.\n8) The movie Red Notice has a runtime of 1 hour and 58 minutes. Approximately how many views did it receive in 2021? Note that Netflix does not provide the weekly_views values that far back in the past, but you can compute it yourself using the total view time and the runtime.\n\n\nCode\nlibrary(lubridate)\nRed_Notice_21 &lt;- GLOBAL_TOP_10 |&gt;\n  filter(show_title == \"Red Notice\", year(week) == 2021) |&gt;\n  summarise(total_hours = sum(weekly_hours_viewed, na.rm = TRUE)) |&gt;\n  mutate(approx_views = total_hours / (1 + 58/60))\n\n\nThe total approximate views Red Notice received in 2021 is 201,732,203 views.\n9) How many Films reached Number 1 in the US but did not originally debut there? That is, find films that first appeared on the Top 10 chart at, e.g., Number 4 but then became more popular and eventually hit Number 1? What is the most recent film to pull this off?\n\n\nCode\ntop_US_films &lt;- COUNTRY_TOP_10 |&gt;\n  filter(country_name == \"United States\", str_detect(category, \"Films\")) |&gt;\n  arrange(week) |&gt;\n  group_by(show_title) |&gt;\n  summarise(\n    debut_week = min(week, na.rm = TRUE),\n    debut_rank = first(weekly_rank),\n    top_rank = min(weekly_rank, na.rm = TRUE),\n    latest_week = max(week)\n    ) |&gt;\n  filter(debut_rank &gt; 1, top_rank == 1)\n\nlastest_film &lt;- top_US_films |&gt; \n  arrange(desc(latest_week)) |&gt;\n  slice(1) \n\ntop_US_films |&gt;\n  format_titles() |&gt;\n  datatable(\n    options = list(searching= FALSE, info= FALSE), \n    caption = \"Films That Climed to Number 1\"\n  )\n\n\n\n\n\n\nCode\ntotal_top_films &lt;- nrow(top_US_films)\nrecent_top_1 &lt;- lastest_film$show_title\n\n\nThere is a total of 45 films that did not originally debut at number 1, but eventually ranked at the top. The most recent film to achieve this was KPop Demon Hunters.\n10) Which TV show/season hit the top 10 in the most countries in its debut week? In how many countries did it chart?\n\n\nCode\ntop_show_debut &lt;- COUNTRY_TOP_10 |&gt;\n  filter(category == \"TV\") |&gt;\n  group_by(show_title) |&gt;\n  filter(week == min(week)) |&gt; \n  summarise(countries_amount = n_distinct(country_name), .groups = \"drop\") |&gt;\n  arrange(desc(countries_amount)) |&gt; \n  slice(1)\n\nshow_name &lt;- top_show_debut$show_title\ncountries_charted &lt;- top_show_debut$countries_amount\n\n\nEmily in Paris hit the Top 10 in most countries in its debut week, with a total of 94 countries charted."
  },
  {
    "objectID": "Untitled.html#the-rise-of-the-upside-down",
    "href": "Untitled.html#the-rise-of-the-upside-down",
    "title": "Mini-Project #01: Gourmet Cheeseburgers Across the Globe",
    "section": "The Rise of the Upside Down",
    "text": "The Rise of the Upside Down\n\nPress Release 1: Upcoming Season of Stranger Things\n\n\n\nStranger Things Season 5\n\n\n\n\nCode\nlibrary(lubridate)\nst_views &lt;- GLOBAL_TOP_10 |&gt;\n  filter(show_title == \"Stranger Things\") |&gt;\n  summarise(total_hours = sum(weekly_hours_viewed, na.rm = TRUE))\n\nst_data &lt;- GLOBAL_TOP_10 %&gt;%\n  filter(str_detect(show_title, regex(\"Stranger Things\", ignore_case = TRUE))) %&gt;%\n  summarise(\n    total_weeks = max(cumulative_weeks_in_top_10, na.rm = TRUE)\n  )\n\ncompare_shows &lt;- GLOBAL_TOP_10 |&gt;\n  select(show_title, weekly_hours_viewed) |&gt; \n  filter(show_title == \"Stranger Things\" | \n           show_title == \"My Life With the Walter Boys\" | \n           show_title ==\"Love Is Blind: UK\" | \n           show_title == \"You\") |&gt; \n  group_by(show_title) |&gt; \n  summarise(total_weekly_hours = sum(weekly_hours_viewed))\n\nwalter_boys &lt;- compare_shows$show_title[2]\nyou_show &lt;- compare_shows$show_title[4]\nwalter_views &lt;- compare_shows$total_weekly_hours[2]\nyou_views &lt;- compare_shows$total_weekly_hours[4]\n\n\nAt last, Stranger Things is coming to an end, releasing its final season on November 26, 2025. Not only has it been a huge hit across the United States, but across other nations as well, garnering a total viewership of 2,967,980,000 hours. Compared to other popular shows such as You and My Life With the Walter Boys, they fell behind with 1,542,990,000 and 629,800,000 hours, respectively. These are astounding numbers, as we saw it was one of the top viewed shows and also ranked highly for several weeks. It has maintained itself 19 weeks in the Top 10.\nAs a Stranger Things watcher myself, I cannot wait to see if Hawkins will be consumed by the underworld, or if it will finally be closed for good."
  },
  {
    "objectID": "Untitled.html#breaking-into-india-netflixs-ambitions-in-the-indian-market",
    "href": "Untitled.html#breaking-into-india-netflixs-ambitions-in-the-indian-market",
    "title": "Mini-Project #01: Gourmet Cheeseburgers Across the Globe",
    "section": "Breaking Into India: Netflix’s Ambitions in the Indian Market",
    "text": "Breaking Into India: Netflix’s Ambitions in the Indian Market\n\nPress Release 2: Commercial Success in India\n\n\nCode\ntop_hindi_films &lt;- GLOBAL_TOP_10 |&gt; select(show_title, weekly_hours_viewed) |&gt; \n  filter(str_detect(show_title, \"Hindi\")) |&gt; \n  group_by(show_title) |&gt; \n  summarise(total_wkly_hrs = sum(weekly_hours_viewed)) |&gt;\n  slice_max(total_wkly_hrs, n = 5)\n  \nhindi_india &lt;- COUNTRY_TOP_10 %&gt;%\n  filter(country_name == \"India\") %&gt;%\n  filter(str_detect(show_title, regex(\"Hindi\", ignore_case = TRUE)) |\n         str_detect(season_title, regex(\"Hindi\", ignore_case = TRUE))) %&gt;%\n  group_by(show_title) %&gt;%\n  summarise(\n    weeks_top10 = max(cumulative_weeks_in_top_10, na.rm = TRUE),\n    peak_rank   = min(weekly_rank, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %&gt;%\n  arrange(peak_rank, desc(weeks_top10))\n\nrrr &lt;- top_hindi_films$show_title[1]\ntop_hindi_2 &lt;- top_hindi_films$show_title[2]\nrrr_hours &lt;-top_hindi_films$total_wkly_hrs[1]\ntop2_hours &lt;- top_hindi_films$total_wkly_hrs[2]\nrrr_rank_weeks &lt;- hindi_india$weeks_top10[1]\n\n\nBeing the most populated country in the world, India is an enticing market many companies are ready to enter and invest in, Netlfix being one of them. Currently RRR (Hindi) and Kalki 2898 AD (Hindi) sit at the top with 79,780,000 and 23,100,000 hours, respectively. Not to mention RRR (Hindi) has spent 25 weeks ranked 1. A total of 11 films have ranked that high, attracting Netflix even more."
  },
  {
    "objectID": "Untitled.html#global-hits-non-english-tv-shows-taking-over-worldwide",
    "href": "Untitled.html#global-hits-non-english-tv-shows-taking-over-worldwide",
    "title": "Mini-Project #01: Gourmet Cheeseburgers Across the Globe",
    "section": "Global Hits: Non-English TV Shows Taking Over Worldwide",
    "text": "Global Hits: Non-English TV Shows Taking Over Worldwide\n\nPress Release 3: Open Topic\n\n\nCode\nne__shows &lt;- GLOBAL_TOP_10 |&gt;\n  filter(category == \"TV (Non-English)\") |&gt;\n  group_by(show_title) |&gt;\n  summarise(\n    total_weeks = max(cumulative_weeks_in_top_10, na.rm = TRUE),\n    total_hours = sum(weekly_hours_viewed, na.rm = TRUE),\n    .groups = \"drop\"\n  ) |&gt;\n  arrange(desc(total_weeks))\n\nne__shows |&gt;\n  mutate(total_hours = scales::comma(total_hours)) |&gt;\n  head(10) |&gt;\n  format_titles() |&gt;\n  datatable(\n    options = list(searching= FALSE, info= FALSE), \n    caption = \"Top Non-English Shows\"\n  )\n\n\n\n\n\n\nCode\nbetty_show &lt;- ne__shows$show_title[2]\nbetty_wks &lt;- ne__shows$total_weeks[2]\nbetty_hours&lt;- ne__shows$total_hours[2]\n\n\nOver the last decade, we have seen an influx of foreign consumption worldwide. The table above demonstrates the TV shows’ amount of weeks spent in the Top 10. Aside from genre, there is also a variety of language in just these five shows, like Korean, Spanish, and Portuguese. Squid Game did extremly well, but Yo soy Betty, la fea did exceptional as well, with 30 weeks in the Top 10 and 297,560,000 hours in viewership. These Korean dramas and Spanish thrillers continue breaking barriers and dominating global charts."
  },
  {
    "objectID": "mp02.html",
    "href": "mp02.html",
    "title": "Making Backyards Affordable for All",
    "section": "",
    "text": "Introduction\nThroughout this mini-project, we will analyze how several factors such as, income, wages, and population, are interconnected with housing affordability. With the help of visualizations, we will be able to see different trends which in turn will help make a decision on who will our next sponsor for YIMBY.\n\n\nData Acquisition\n\nTask 1: Data Import\n\n\nShow Data Code\nif(!dir.exists(file.path(\"data\", \"mp02\"))){\n    dir.create(file.path(\"data\", \"mp02\"), showWarnings=FALSE, recursive=TRUE)\n}\n\nlibrary &lt;- function(pkg){\n    ## Mask base::library() to automatically install packages if needed\n    ## Masking is important here so downlit picks up packages and links\n    ## to documentation\n    pkg &lt;- as.character(substitute(pkg))\n    options(repos = c(CRAN = \"https://cloud.r-project.org\"))\n    if(!require(pkg, character.only=TRUE, quietly=TRUE)) install.packages(pkg)\n    stopifnot(require(pkg, character.only=TRUE, quietly=TRUE))\n}\n\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nShow Data Code\nlibrary(glue)\nlibrary(readxl)\nlibrary(tidycensus)\n\nget_acs_all_years &lt;- function(variable, geography=\"cbsa\",\n                              start_year=2009, end_year=2023){\n    fname &lt;- glue(\"{variable}_{geography}_{start_year}_{end_year}.csv\")\n    fname &lt;- file.path(\"data\", \"mp02\", fname)\n    \n    if(!file.exists(fname)){\n        YEARS &lt;- seq(start_year, end_year)\n        YEARS &lt;- YEARS[YEARS != 2020] # Drop 2020 - No survey (covid)\n        \n        ALL_DATA &lt;- map(YEARS, function(yy){\n            tidycensus::get_acs(geography, variable, year=yy, survey=\"acs1\") |&gt;\n                mutate(year=yy) |&gt;\n                select(-moe, -variable) |&gt;\n                rename(!!variable := estimate)\n        }) |&gt; bind_rows()\n        \n        write_csv(ALL_DATA, fname)\n    }\n    \n    read_csv(fname, show_col_types=FALSE)\n}\n\n# Household income (12 month)\nINCOME &lt;- get_acs_all_years(\"B19013_001\") |&gt;\n    rename(household_income = B19013_001)\n\n# Monthly rent\nRENT &lt;- get_acs_all_years(\"B25064_001\") |&gt;\n    rename(monthly_rent = B25064_001)\n\n# Total population\nPOPULATION &lt;- get_acs_all_years(\"B01003_001\") |&gt;\n    rename(population = B01003_001)\n\n# Total number of households\nHOUSEHOLDS &lt;- get_acs_all_years(\"B11001_001\") |&gt;\n    rename(households = B11001_001)\n\n\nAdditionally, the number of new housing units built each year is necessary for this report.\n\n\nCode\nget_building_permits &lt;- function(start_year = 2009, end_year = 2023){\n    fname &lt;- glue(\"housing_units_{start_year}_{end_year}.csv\")\n    fname &lt;- file.path(\"data\", \"mp02\", fname)\n    \n    if(!file.exists(fname)){\n        HISTORICAL_YEARS &lt;- seq(start_year, 2018)\n        \n        HISTORICAL_DATA &lt;- map(HISTORICAL_YEARS, function(yy){\n            historical_url &lt;- glue(\"https://www.census.gov/construction/bps/txt/tb3u{yy}.txt\")\n                \n            LINES &lt;- readLines(historical_url)[-c(1:11)]\n\n            CBSA_LINES &lt;- str_detect(LINES, \"^[[:digit:]]\")\n            CBSA &lt;- as.integer(str_sub(LINES[CBSA_LINES], 5, 10))\n\n            PERMIT_LINES &lt;- str_detect(str_sub(LINES, 48, 53), \"[[:digit:]]\")\n            PERMITS &lt;- as.integer(str_sub(LINES[PERMIT_LINES], 48, 53))\n            \n            data_frame(CBSA = CBSA,\n                       new_housing_units_permitted = PERMITS, \n                       year = yy)\n        }) |&gt; bind_rows()\n        \n        CURRENT_YEARS &lt;- seq(2019, end_year)\n        \n        CURRENT_DATA &lt;- map(CURRENT_YEARS, function(yy){\n            current_url &lt;- glue(\"https://www.census.gov/construction/bps/xls/msaannual_{yy}99.xls\")\n            \n            temp &lt;- tempfile()\n            \n            download.file(current_url, destfile = temp, mode=\"wb\")\n            \n            fallback &lt;- function(.f1, .f2){\n                function(...){\n                    tryCatch(.f1(...), \n                             error=function(e) .f2(...))\n                }\n            }\n            \n            reader &lt;- fallback(read_xlsx, read_xls)\n            \n            reader(temp, skip=5) |&gt;\n                na.omit() |&gt;\n                select(CBSA, Total) |&gt;\n                mutate(year = yy) |&gt;\n                rename(new_housing_units_permitted = Total)\n        }) |&gt; bind_rows()\n        \n        ALL_DATA &lt;- rbind(HISTORICAL_DATA, CURRENT_DATA)\n        \n        write_csv(ALL_DATA, fname)\n        \n    }\n    \n    read_csv(fname, show_col_types=FALSE)\n}\n\nPERMITS &lt;- get_building_permits()\n\n\nUsing the following code will allow us refer to useful income estimates from the Bureau of Labor Statistics(BLS). This code fill download such data from the North American Industry Classification System (NAICS). Ultimately, the goal is to know the best Core-Based Statistical Areas (CBSA) to live in.\n\n\nCode\nlibrary(httr2)\nlibrary(rvest)\nget_bls_industry_codes &lt;- function(){\n    fname &lt;- file.path(\"data\", \"mp02\", \"bls_industry_codes.csv\")\n    library(dplyr)\n    library(tidyr)\n    library(readr)\n    \n    if(!file.exists(fname)){\n        \n        resp &lt;- request(\"https://www.bls.gov\") |&gt; \n            req_url_path(\"cew\", \"classifications\", \"industry\", \"industry-titles.htm\") |&gt;\n            req_headers(`User-Agent` = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:143.0) Gecko/20100101 Firefox/143.0\") |&gt; \n            req_error(is_error = \\(resp) FALSE) |&gt;\n            req_perform()\n        \n        resp_check_status(resp)\n        \n        naics_table &lt;- resp_body_html(resp) |&gt;\n            html_element(\"#naics_titles\") |&gt; \n            html_table() |&gt;\n            mutate(title = str_trim(str_remove(str_remove(`Industry Title`, Code), \"NAICS\"))) |&gt;\n            select(-`Industry Title`) |&gt;\n            mutate(depth = if_else(nchar(Code) &lt;= 5, nchar(Code) - 1, NA)) |&gt;\n            filter(!is.na(depth))\n        \n        # These were looked up manually on bls.gov after finding \n        # they were presented as ranges. Since there are only three\n        # it was easier to manually handle than to special-case everything else\n        naics_missing &lt;- tibble::tribble(\n            ~Code, ~title, ~depth, \n            \"31\", \"Manufacturing\", 1,\n            \"32\", \"Manufacturing\", 1,\n            \"33\", \"Manufacturing\", 1,\n            \"44\", \"Retail\", 1, \n            \"45\", \"Retail\", 1,\n            \"48\", \"Transportation and Warehousing\", 1, \n            \"49\", \"Transportation and Warehousing\", 1\n        )\n        \n        naics_table &lt;- bind_rows(naics_table, naics_missing)\n        \n        naics_table &lt;- naics_table |&gt; \n            filter(depth == 4) |&gt; \n            rename(level4_title=title) |&gt; \n            mutate(level1_code = str_sub(Code, end=2), \n                   level2_code = str_sub(Code, end=3), \n                   level3_code = str_sub(Code, end=4)) |&gt;\n            left_join(naics_table, join_by(level1_code == Code)) |&gt;\n            rename(level1_title=title) |&gt;\n            left_join(naics_table, join_by(level2_code == Code)) |&gt;\n            rename(level2_title=title) |&gt;\n            left_join(naics_table, join_by(level3_code == Code)) |&gt;\n            rename(level3_title=title) |&gt;\n            select(-starts_with(\"depth\")) |&gt;\n            rename(level4_code = Code) |&gt;\n            select(level1_title, level2_title, level3_title, level4_title, \n                   level1_code,  level2_code,  level3_code,  level4_code) |&gt;\n            drop_na() |&gt;\n            mutate(across(contains(\"code\"), as.integer))\n        \n        write_csv(naics_table, fname)\n    }\n    \n    read_csv(fname, show_col_types=FALSE)\n}\n\nINDUSTRY_CODES &lt;- get_bls_industry_codes()\n\n\nFinally, we need to obtain the BLS Quarterly Census of Employment and Wages.\n\n\nCode\nlibrary(httr2)\nlibrary(rvest)\nget_bls_qcew_annual_averages &lt;- function(start_year=2009, end_year=2023){\n    fname &lt;- glue(\"bls_qcew_{start_year}_{end_year}.csv.gz\")\n    fname &lt;- file.path(\"data\", \"mp02\", fname)\n    \n    YEARS &lt;- seq(start_year, end_year)\n    YEARS &lt;- YEARS[YEARS != 2020] # Drop Covid year to match ACS\n    \n    if(!file.exists(fname)){\n        ALL_DATA &lt;- map(YEARS, .progress=TRUE, possibly(function(yy){\n            fname_inner &lt;- file.path(\"data\", \"mp02\", glue(\"{yy}_qcew_annual_singlefile.zip\"))\n            \n            if(!file.exists(fname_inner)){\n                request(\"https://www.bls.gov\") |&gt; \n                    req_url_path(\"cew\", \"data\", \"files\", yy, \"csv\",\n                                 glue(\"{yy}_annual_singlefile.zip\")) |&gt;\n                    req_headers(`User-Agent` = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:143.0) Gecko/20100101 Firefox/143.0\") |&gt; \n                    req_retry(max_tries=5) |&gt;\n                    req_perform(fname_inner)\n            }\n            \n            if(file.info(fname_inner)$size &lt; 755e5){\n                warning(sQuote(fname_inner), \"appears corrupted. Please delete and retry this step.\")\n            }\n            \n            read_csv(fname_inner, \n                     show_col_types=FALSE) |&gt; \n                mutate(YEAR = yy) |&gt;\n                select(area_fips, \n                       industry_code, \n                       annual_avg_emplvl, \n                       total_annual_wages, \n                       YEAR) |&gt;\n                filter(nchar(industry_code) &lt;= 5, \n                       str_starts(area_fips, \"C\")) |&gt;\n                filter(str_detect(industry_code, \"-\", negate=TRUE)) |&gt;\n                mutate(FIPS = area_fips, \n                       INDUSTRY = as.integer(industry_code), \n                       EMPLOYMENT = as.integer(annual_avg_emplvl), \n                       TOTAL_WAGES = total_annual_wages) |&gt;\n                select(-area_fips, \n                       -industry_code, \n                       -annual_avg_emplvl, \n                       -total_annual_wages) |&gt;\n                # 10 is a special value: \"all industries\" , so omit\n                filter(INDUSTRY != 10) |&gt; \n                mutate(AVG_WAGE = TOTAL_WAGES / EMPLOYMENT)\n        })) |&gt; bind_rows()\n        \n        write_csv(ALL_DATA, fname)\n    }\n    \n    ALL_DATA &lt;- read_csv(fname, show_col_types=FALSE)\n    \n    ALL_DATA_YEARS &lt;- unique(ALL_DATA$YEAR)\n    \n    YEARS_DIFF &lt;- setdiff(YEARS, ALL_DATA_YEARS)\n    \n    if(length(YEARS_DIFF) &gt; 0){\n        stop(\"Download failed for the following years: \", YEARS_DIFF, \n             \". Please delete intermediate files and try again.\")\n    }\n    \n    ALL_DATA\n}\n\nWAGES &lt;- get_bls_qcew_annual_averages()\n\n\n\n\n\n\nData Integration and Initial Exploration\n\n\n\nRelationship Diagram\n\n\n\nTask 2: Multi-Table Questions\n1. Which CBSA (by name) permitted the largest number of new housing units in the decade from 2010 to 2019 (inclusive)?\n\n\nCode\nlibrary(dplyr)\n\nnew_housing_units1019 &lt;- PERMITS |&gt;\n  filter(year &gt;= 2010, year &lt;= 2019) |&gt;\n  group_by(CBSA) |&gt;\n  summarise(total_permits_given = sum(new_housing_units_permitted, na.rm = TRUE)) |&gt;\n  arrange(desc(total_permits_given)) |&gt;\n  left_join(POPULATION |&gt; select(GEOID, NAME) |&gt; distinct(), \n            by = c(\"CBSA\" = \"GEOID\")) |&gt;\n  ungroup()\n\ntop_cities &lt;- new_housing_units1019 |&gt; arrange(desc(total_permits_given)) |&gt; slice_max(total_permits_given, n=10)\n  \nlibrary(ggplot2)\nggplot(top_cities, aes(x = reorder(NAME, total_permits_given), y = total_permits_given)) +\n  geom_col(fill = \"steelblue\", width = 0.5) +\n  coord_flip() +\n  scale_y_continuous(labels = scales::comma) +\n  labs(\n    title = \"Total New Housing Units Permited (2010-2019)\",\n    x = \"Metropolitan Area (CBSA)\",\n    y = \"Total Permits Issued\"\n  ) +\n  theme_minimal(base_size = 9)\n\n\n\n\n\n\n\n\n\nCode\ntop_city &lt;- new_housing_units1019$total_permits_given[1]\ntop_city_name &lt;- new_housing_units1019$NAME[1]\n\n\nThe largest number of new housing units in the decade from 2010 to 2019 was in Houston-Sugar Land-Baytown, TX Metro Area with a total of 482,075 permits issued.\n2. In what year did Albuquerque, NM (CBSA Number 10740) permit the most new housing units?\n\n\nCode\nlibrary(DT)\n\nalbuquerque_new_permits &lt;- PERMITS |&gt; \n  filter(CBSA == 10740) |&gt;\n  group_by(year) |&gt;\n  summarise(ABQ_total_permits = sum(new_housing_units_permitted, na.rm = TRUE)) |&gt;\n  arrange(desc(ABQ_total_permits)) |&gt;\n  ungroup()\n\nggplot(albuquerque_new_permits, aes(x = year, y = ABQ_total_permits)) +\n  geom_col(fill = \"steelblue\") +\n  labs(\n    title = \"Albuquerque New Housing Units Permitted by Year\",\n    x = \"Year\",\n    y = \"Total Permits\"\n  ) +\n  theme_minimal(base_size = 10)\n\n\n\n\n\n\n\n\n\nCode\nabq_newest_top_year &lt;- albuquerque_new_permits$year[1]\nabq_new_total_permits &lt;- albuquerque_new_permits$ABQ_total_permits[1]\n\n\nIn Albuquerque, NM, 2021 was the year in which new housing units were permitted the most, with a total of 4,021 permits issued.\n3. Which state (not CBSA) had the highest average individual income in 2015?\n\n\nCode\nlibrary(dplyr)\nhighest_avg_ind_income &lt;- INCOME |&gt;\n  filter(year == 2015) |&gt;\n  left_join(HOUSEHOLDS, \n            by = c(\"GEOID\", \"NAME\", \"year\")) |&gt;\n  left_join(POPULATION, \n            by = c(\"GEOID\", \"NAME\", \"year\")) |&gt;\n  mutate(state = str_extract(NAME, \", (.{2})\", group=1),\n         total_income_CBSA = household_income*households) |&gt;\n  group_by(state) |&gt; \n  summarise(\n    state_total_income = sum(total_income_CBSA, na.rm = TRUE),\n    total_population = sum(population, na.rm = TRUE), \n    average_individual_income = state_total_income/total_population\n  ) |&gt;\n  arrange(desc(average_individual_income)) \n\nlibrary(scales)\nlibrary(DT)\n\ntop_states &lt;- highest_avg_ind_income |&gt; \n  arrange(desc(average_individual_income)) |&gt; \n  slice_max(average_individual_income, n = 5) |&gt; \n  mutate(\n    state_total_income = dollar(state_total_income),\n    total_population = comma(total_population),\n    average_individual_income = dollar(average_individual_income)\n  )\n\ncolnames(top_states) &lt;- str_replace_all(colnames(top_states), \"_\", \" \") |&gt;\n  str_to_title()\n\ndatatable(\n  top_states,\n  options = list(searching = FALSE, info = FALSE),\n  caption = \"Top 5 Highest Average Individual Income by State (2015)\"\n)\n\n\n\n\n\n\nThe state with the highest average individual income is DC, with an average of $33,232.88.\n4. Data scientists and business analysts are recorded under NAICS code 5182. What is the last year in which the NYC CBSA had the most data scientists in the country? In recent, the San Francisco CBSA has had the most data scientists.\n\n\nCode\nlibrary(dplyr)\nfiltered_ind &lt;- WAGES |&gt;\n  filter(INDUSTRY == 5182) |&gt; \n  group_by(YEAR, FIPS) |&gt;\n  summarise(total_employment = sum(EMPLOYMENT, na.rm = TRUE)) |&gt;\n  filter(total_employment == max(total_employment)) |&gt;\n  mutate(CBSA = paste0(FIPS, \"0\"))\n\nfiltered_population &lt;- POPULATION |&gt;\n  mutate(CBSA = paste0(\"C\", GEOID))\n\nlibrary(DT)\nlibrary(scales)\nData_Scientist &lt;- inner_join(\n  filtered_ind,\n  filtered_population,\n  join_by(CBSA == CBSA, YEAR == year))|&gt; \n  group_by(YEAR, NAME) |&gt; select(-population, -FIPS, -GEOID) |&gt;\n  filter(str_detect(NAME, \"New York\")) |&gt;\n  arrange(desc(total_employment)) |&gt; \n  mutate(total_employment = comma(total_employment))\n\ntop_ds_year &lt;- Data_Scientist$Year[1]\ntop_ds_te &lt;- Data_Scientist$`Total Employment`[1]\n\ncolnames(Data_Scientist) &lt;- str_replace_all(colnames(Data_Scientist), \"_\", \" \") |&gt;\n  str_to_title()\n\ndatatable(\n  Data_Scientist,\n  options = list(searching = FALSE, info = FALSE),\n  caption = \"NYC CBSA Data Scientists\"\n)\n\n\n\n\n\n\nThe last year in which the NYC CBSA had the most data scientists in the country was in 2015, with a total of 18,922 employees.\n5. What fraction of total wages in the NYC CBSA was earned by people employed in the finance and insurance industries (NAICS code 52)? In what year did this fraction peak?\n\n\nCode\nwages_nyc &lt;- WAGES |&gt;\n  filter(FIPS == \"C3562\") |&gt; \n  group_by(YEAR) |&gt;\n  summarise(total_wages_nyc = sum(TOTAL_WAGES, na.rm = TRUE),\n            finance_and_insurance_wages = sum(ifelse(INDUSTRY == 52, TOTAL_WAGES,0)),\n            finance_fraction= finance_and_insurance_wages/total_wages_nyc)|&gt;\n  mutate(\n    total_wages_nyc = dollar(total_wages_nyc),\n    finance_and_insurance_wages = dollar(finance_and_insurance_wages),\n    finance_fraction = percent(finance_fraction)) |&gt;\n  arrange(desc(finance_fraction))\n\ncolnames(wages_nyc) &lt;- str_replace_all(colnames(wages_nyc), \"_\", \" \") |&gt;\n  str_to_title()\n\ndatatable(\n  wages_nyc,\n  options = list(searching = FALSE, info = FALSE),\n  caption = \"Finance and Insurance Wages and Fractions\"\n)\n\n\n\n\n\n\nCode\npeak_year &lt;- wages_nyc$Year[1]\npeak_fraction &lt;- wages_nyc$`Finance Fraction`[1]\n\n\nApproximately 4.6038% of total wages was earned by people employed in the finance and insurance industries, and they peaked in 2014.\n\n\nTask 3: Initial Visualizations\nVisualization 1\n\n\n\n\n\n\n\n\n\nVisualization 2\n\n\nCode\nhealth_employment &lt;- WAGES |&gt;\n  mutate(healthcare = INDUSTRY == 62, \n         cbsa = paste0(FIPS, \"0\")) |&gt;\n  group_by(cbsa,YEAR) |&gt;\n  summarise(tt_hh_ss_employment = sum(EMPLOYMENT[healthcare], na.rm = TRUE),\n            total_employment = sum(EMPLOYMENT, na.rm = TRUE), .groups = \"drop\") |&gt;\n  filter(total_employment &gt; 0, tt_hh_ss_employment &gt; 0)\n\nlibrary(ggplot2)\nlibrary(scales)\nggplot(health_employment, aes(x = total_employment/1000, y = tt_hh_ss_employment/1000)) +\n  geom_point(aes(color = cbsa),alpha = 0.2, show.legend = FALSE) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red4\", linewidth = 0.7) +\n  facet_wrap(~YEAR, ncol = 3)+\n  labs(\n    title = \"Healthcare Employment vs Total Employment\",\n    subtitle = \"Across Different CBSAs\",\n    x = \"Total Employment\",\n    y = \"Total H and S Employment\",\n    caption = \"idl yet\"\n  ) +\n  scale_x_continuous(labels = comma_format()) +\n  scale_y_continuous(labels = comma_format()) +\n  theme_minimal(base_size = 10) +\n  theme_bw() + \n  theme(\n    plot.title = element_text(face = \"bold\"))\n\n\n\n\n\n\n\n\n\nVisualization 3\n\n\nCode\nHousehold_trend &lt;- POPULATION |&gt;\n  mutate(CBSA = paste0(GEOID, \"0\")) |&gt;\n  inner_join(HOUSEHOLDS, by = c(\"GEOID\", \"NAME\", \"year\")) |&gt;\n  mutate(avg_hh_size = population/households) |&gt;\n  arrange(desc(avg_hh_size))\n\nlibrary(gghighlight)\n  \nggplot(Household_trend, aes(\n  x = year, y = avg_hh_size,\n  group = CBSA, color = case_when(CBSA == 356200 ~ \"NY-NJ Metro Area\",\n                                  CBSA == 310800 ~ \"CA Metro Area\")\n)) +\n  geom_line() +\n  scale_color_manual(values = c(\"NY-NJ Metro Area\" = \"steelblue\", \"CA Metro Area\" = \"firebrick\"),\n                     name = \"Highlight\") +\n  gghighlight(CBSA %in% c(356200,310800), \n              use_direct_label = FALSE, \n              unhighlighted_params = list(alpha = 0.3)) +\n  scale_x_continuous(\n    breaks = seq(2009, 2023, 2)\n  ) +\n  labs(\n    title = \"Evolution of Average Household Size Over Time (2009-2023)\",\n    subtitle = \"Across Different CBSAs\",\n    x = \"Year\",\n    y = \"Average Household Size\",\n    caption = \"will come up iwth one\"\n  ) +\n  theme_bw() +\n  theme(\n    plot.title = element_text(face = \"bold\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding Indices of Housing Affordability and housing Stock Growth\n\nTask 4: Rent Burden\n\n\nCode\nlibrary(dplyr)\nlibrary(DT)\nrent_burden &lt;- INCOME |&gt;\n  inner_join(RENT, by = c(\"GEOID\", \"NAME\", \"year\")) |&gt;\n  inner_join(POPULATION, by = c(\"GEOID\", \"NAME\", \"year\"))|&gt;\n  filter(!is.na(household_income), !is.na(monthly_rent)) |&gt;\n  mutate(\n    rent_income_percentage = (monthly_rent * 12 / household_income)) |&gt; \n  select(-household_income, \n         -monthly_rent) |&gt;\n  mutate(rent_burden_index = scales::rescale(rent_income_percentage, to = c(0, 100))) \n\ncolnames(rent_burden) &lt;- str_replace_all(colnames(rent_burden), \"_\", \" \") |&gt;\n  str_to_title()\n\n\nhighest_burden &lt;- max(rent_burden$`Rent Income Percentage`, na.rm = TRUE)\nlowest_burden &lt;- min(rent_burden$`Rent Income Percentage`, na.rm = TRUE)\nhighest_name &lt;- rent_burden$Name[4976]\nlowest_name &lt;- rent_burden$Name[49]\n\n#Ohio Metro Area\nohio_metro_area &lt;- rent_burden |&gt; filter(Geoid == 26580)\n\ndatatable(\n  ohio_metro_area,\n  options = list(searching = FALSE, info = FALSE),\n  caption = \"Ohio Rent Burden\" ) |&gt; formatPercentage(columns = \"Rent Income Percentage\", digits = 2) |&gt;\n  formatRound(columns = \"Rent Burden Index\", digits = 2)\n\n\n\n\n\n\nthis is my second table highlighting metro area.\n\n\nCode\nhighest &lt;- rent_burden |&gt; \n  filter(`Rent Income Percentage` == max(`Rent Income Percentage`, na.rm = TRUE)) |&gt; \n  mutate(Level = \"Highest Burden\")\n\nlowest &lt;- rent_burden |&gt; \n  filter(`Rent Income Percentage` == min(`Rent Income Percentage`, na.rm = TRUE)) |&gt; \n  mutate(Level = \"Lowest Burden\")\n\nburden_summary &lt;- bind_rows(highest, lowest) |&gt; \n  select(Level, Name, Geoid, Year, `Rent Income Percentage`, `Rent Burden Index`)\n\ndatatable(\n  burden_summary,\n  caption = \"Areas with Highest and Lowest Rent Burden\",\n  options = list(pageLength = 5, searching = FALSE, info = FALSE)\n) |&gt;\n  formatPercentage(columns = \"Rent Income Percentage\", digits = 2) |&gt;\n  formatRound(columns = \"Rent Burden Index\", digits = 2)\n\n\n\n\n\n\n\n\nTask 5: Housing Growth\n\n\nCode\nlibrary(dplyr)\nlibrary(RcppRoll)\n\n# 1. Join POPULATION and PERMITS tables\nhousing_growth &lt;- POPULATION %&gt;%\n  inner_join(PERMITS, join_by(GEOID == CBSA, year == year)) %&gt;%\n  arrange(GEOID, year) %&gt;%\n  group_by(GEOID) %&gt;%\n  mutate(\n    # 5-year lagged population and population growth\n    population_5yr_ago = lag(population, 5),\n    population_growth_5yr = population - population_5yr_ago,\n    base_year = year &gt;= 2014\n  ) %&gt;%\n  ungroup()\n\n# 2. Compute national averages for standardization\nnational_stats &lt;- list(\n  permits_mean = mean(housing_growth$new_housing_units_permitted, na.rm = TRUE),\n  pop_mean = mean(housing_growth$population, na.rm = TRUE),\n  growth_mean = mean(housing_growth$population_growth_5yr[housing_growth$base_year], na.rm = TRUE)\n)\n\n# 3. Construct metrics\nhousing_growth &lt;- housing_growth %&gt;%\n  mutate(\n    # --- Instantaneous Metric ---\n    # Scale by population and national average\n    permits_per_1000_pop = (new_housing_units_permitted / population) * 1000,\n    national_permits_per_1000 = (national_stats$permits_mean / national_stats$pop_mean) * 1000,\n    instantaneous_metric = (permits_per_1000_pop / national_permits_per_1000) * 50,\n    \n    # --- Rate-Based Metric ---\n    # Ratio of permits to 5-year population growth, standardized to national average\n    permits_to_growth_ratio = new_housing_units_permitted / pmax(population_growth_5yr),\n    rate_based_metric = if_else(\n      base_year,\n      (permits_to_growth_ratio / mean(permits_to_growth_ratio, na.rm = TRUE)) * 50,\n      NA_real_\n    ),\n    \n    # --- Composite Metric ---\n    composite_index = if_else(\n      base_year,\n      (instantaneous_metric + rate_based_metric) / 2,\n      NA_real_\n    )\n  ) %&gt;%\n  group_by(GEOID) %&gt;%\n  # 5-year rolling average of composite metric\n  mutate(composite_index_rolling = roll_mean(composite_index, n = 5, align = \"right\", fill = NA)) %&gt;%\n  ungroup()\n\nlibrary(dplyr)\nlibrary(DT)\n\n# Create simplified Instantaneous Metric Table\ninstantaneous_table &lt;- housing_growth %&gt;%\n  select(\n    CBSA = GEOID,\n    Name = NAME,       # Adjust if your name column is different\n    year,\n    permits_per_1000_pop,\n    instantaneous_metric\n  ) %&gt;%\n  arrange(desc(permits_per_1000_pop))\n\ncolnames(instantaneous_table) &lt;- str_replace_all(colnames(instantaneous_table), \"_\", \" \") |&gt;\n  str_to_title()\n\n# Display interactive DT table\ndatatable(\n  instantaneous_table,\n  rownames = FALSE,\n  filter = \"top\",\n  options = list(pageLength = 15, autoWidth = TRUE, scrollX = TRUE),\n  caption = \"CBSA Instantaneous Housing Growth Metrics\"\n)   |&gt; formatRound(columns = \"Permits Per 1000 Pop\", digits = 2) |&gt; formatRound(columns = \"Instantaneous Metric\", digits = 2)\n\n\n\n\n\n\nThe Instantaneous index allows us to see the permits per 1,000 residents, which ultimately tells us how active construction is right now.\n\n\nCode\nlibrary(dplyr)\nlibrary(DT)\n\n# Create simplified Rate-Based Metric Table\nrate_based_table &lt;- housing_growth %&gt;%\n  filter(base_year) %&gt;%  # Only include years with 5-year growth\n  select(\n    CBSA = GEOID,\n    Name = NAME,   # Adjust if your column has a different name\n    year,\n    permits_to_growth_ratio,\n    rate_based_metric\n  ) %&gt;%\n  arrange(desc(permits_to_growth_ratio))\n\ncolnames(rate_based_table) &lt;- str_replace_all(colnames(rate_based_table), \"_\", \" \") |&gt;\n  str_to_title()\n\n# Display interactive DT table\ndatatable(\n  rate_based_table,\n  rownames = FALSE,\n  filter = \"top\",\n  options = list(pageLength = 15, autoWidth = TRUE, scrollX = TRUE),\n  caption = \"CBSA Rate-Based Housing Growth Metrics\"\n) |&gt; formatRound(columns = \"Permits To Growth Ratio\", digits = 2) |&gt; formatRound(columns = \"Rate Based Metric\", digits = 2)\n\n\n\n\n\n\nRate-based index indicated a population growth per 5-year which tells us how well supply tracks demand.\n\n\nTask 6: Visualization\n\n\nCode\nlibrary(dplyr)\n\n# Make sure column names match your datasets\nrent_burden_clean &lt;- rent_burden |&gt; \n  rename(GEOID = Geoid, year = Year, rent_burden_index = `Rent Burden Index`, \n         rent_income_percentage = `Rent Income Percentage`, Name = Name)\n\n# Merge rent burden and housing growth\ncombined &lt;- housing_growth %&gt;%\n  inner_join(rent_burden_clean, by = c(\"NAME\" = \"Name\", \"GEOID\" = \"GEOID\", \"year\" = \"year\")\n) %&gt;%\n  group_by(GEOID, NAME) %&gt;%\n  mutate(\n    rent_burden_change = rent_burden_index - lag(rent_burden_index, 5),\n    population_change = population - lag(population, 5)\n  ) %&gt;%\n  ungroup()\n\nlibrary(ggplot2)\n\n# Calculate early rent burden (average in first 5 years)\nearly_burden &lt;- combined %&gt;%\n  filter(year &lt;= min(year) + 4) %&gt;%\n  group_by(GEOID, NAME) %&gt;%\n  summarize(avg_rent_burden_early = mean(rent_burden_index, na.rm = TRUE)) %&gt;%\n  ungroup()\n\n# Join back to get one row per CBSA for latest year\nplot_data &lt;- combined %&gt;%\n  filter(year == max(year)) %&gt;%\n  inner_join(early_burden, by = c(\"GEOID\", \"NAME\"))\n\nggplot(plot_data, aes(\n  x = composite_index_rolling,\n  y = rent_burden_change,\n  size = population_change,\n  color = avg_rent_burden_early\n)) +\n  geom_point(alpha = 0.7) +\n  scale_color_viridis_c(option = \"plasma\", name = \"Early Rent Burden\") +\n  scale_size_continuous(name = \"Population Growth\") +\n  labs(\n    title = \"Rent Burden Change vs. Housing Growth (YIMBY Analysis)\",\n    subtitle = \"CBSAs with high early rent burden, decreasing rents, and strong housing growth are YIMBY successes\",\n    x = \"Composite Housing Growth Index (5-Year Rolling Avg)\",\n    y = \"Change in Rent Burden (↓ = Rent Relief)\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Identify YIMBY candidate CBSAs\nyimby_candidates &lt;- plot_data %&gt;%\n  filter(\n    avg_rent_burden_early &gt; mean(avg_rent_burden_early, na.rm = TRUE),\n    rent_burden_change &lt; 0,\n    composite_index_rolling &gt; mean(composite_index_rolling, na.rm = TRUE),\n    population_change &gt; 0\n  ) %&gt;%\n  arrange(rent_burden_change) %&gt;%\n  slice_head(n = 5) %&gt;%\n  pull(GEOID)\n\n# Time series for selected CBSAs\nggplot(combined %&gt;% filter(GEOID %in% yimby_candidates), aes(x = year)) +\n  geom_line(aes(y = rent_burden_index, color = \"Rent Burden\"), size = 0.6) +\n  geom_line(aes(y = composite_index_rolling/2, color = \"Housing Growth (Scaled)\"), size = 0.6) +\n  facet_wrap(~NAME, scales = \"free_y\") +\n  scale_color_manual(values = c(\"Rent Burden\" = \"red\", \"Housing Growth (Scaled)\" = \"blue\")) +\n  labs(\n    title = \"Rent Burden vs. Housing Growth Over Time\",\n    subtitle = \"YIMBY CBSAs show rising housing growth and declining rent burden\",\n    y = \"Index (Scaled)\",\n    color = \"Metric\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\nPolicy Brief\n\nTask 7\n\nTitle: Adopting More YIMBY Housing Policies\n\n\nExecutive Summary\nThrough our findings, we were able to find correlations between housing, wages, income, industries, rent, and population. Overall, we discovered the relationships between rent burden, housing growth and population growth. The goal of the policy is to refer to cities which will actively grow in the sectors of housing, those whose rent will not increase to unattainable standards, and will encourage outsiders to move in, supporting a ethical and sustainable growth.\n\n\nProposed Congressional Sponsors:\nThere are multiple YIMBY success candidates that we found across our research, as we can see in the previous graph highlighting rising in housing growth and decrease in rent burden. The primary sponsor we will be choosing is Gulfport-Biloxi, MS Metro Area, which had the least burden changes, and actually decreasing it, with a negative percentage of -15.9170722%.\nOur cosponsor would preferably be a city which we can work from scratch. There are cities whose housing development does not seem to increase, while the rent keeps increasing, leaving locals with a heavy burden. In Texarkana, TX-AR Metro Area, the rent burden keeps increasing with a 22.3293759% while population appears to be decreasing by -5825 in 5 years.\n\n\nRecommendation\nOur polices should be implemented into either city as they will make one city continue to thrive, while the other will begin to flourish. Locals deserve to stay in their home, especially as natives, yet unfortunately some are forcefully relocated elsewhere due to high rent prices. That is why we need the help of not only the Congress, but also of the people. We encourage all local municipalities to allow us to cooperate with them to transform their home into a greater place.\n\nThe metrics we use are based on Rent Burden Index, a Housing Growth Index, and Household Trends, to name a few. These allow us to see any changes from 2009-2023.\n\n\n\nAllow us to help you live and thrive!"
  },
  {
    "objectID": "mp02.html#task-1",
    "href": "mp02.html#task-1",
    "title": "Making Backyards Affordable for All",
    "section": "Task 1",
    "text": "Task 1"
  },
  {
    "objectID": "mp03.html",
    "href": "mp03.html",
    "title": "Visualizing and Maintaining the Green Canopy of NYC",
    "section": "",
    "text": "The Incredible Environmental Benefits of NYC Trees | Craig. D.J. (2023)\n\n\n\nIntroduction\nGreenery is a city quality that will always be cherished among urban cities, especially in New York City, where we are surrounded by a jungle of concrete. Not only is it visually pleasing, but it is also home to many species, provides better air quality, and brings together the community. By obtaining data from NYC TreeMap dataset, we are able to get insight on any major issues relating to this urban jungle.\n\n\n\nData Acquisition\n\nNYC City Council Districts\nIn order to obtain information about trees in each council, we must download data about NYC’s 51 council districts and its boundaries.\n\n\n\n\n\n\nNoteTask 1: Download NYC Council District Boundaries\n\n\n\n\n\n\n\n\nCode to download data\nlibrary(sf)\nlibrary(fs)\nlibrary(tidyverse)\nlibrary(httr2)\nlibrary(glue)\nlibrary(dplyr)\n\nif(!dir.exists(file.path(\"data\", \"mp03\"))){\n    dir.create(file.path(\"data\", \"mp03\"), showWarnings=FALSE, recursive=TRUE)\n}\n\nNYC_CC_2025 &lt;- function(){\n  mp03 &lt;- file.path(\"data\", \"mp03\")\n  \n  fname&lt;- \"nycc_25c.zip\"\n  zip_path &lt;- file.path(mp03, fname)\n  \n  city_url &lt;- glue(\"https://s-media.nyc.gov/agencies/dcp/assets/files/zip/data-tools/bytes/city-council/nycc_25c.zip\")\n  \n  if(!file.exists(zip_path)){\n    download.file(city_url, destfile = zip_path, mode = \"wb\")\n    message(glue(\"Downloaded {fname}\"))\n  } else {\n    message(glue(\"{fname} already exists, skipping download.\"))\n  }\n  \n  shp_file &lt;- dir_ls(mp03, recurse = TRUE, glob = \"*.shp\")\n  if(length(shp_file) == 0) {\n    message(\"no shapefile found - unzipping archive...\")\n    unzip(zip_path, exdir = mp03)\n    shp_file &lt;- dir_ls(mp03, recurse = TRUE, glob = \"*.shp\")\n  } else {\n    message(\"Shapefile already extracted - skipping unzip.\")\n  }\n  \n  message(\"Reading shapefile and transforming to WGS 84...\")\n  NYC_CC_file &lt;- st_read(shp_file[1], quiet = TRUE)\n  NYC_CC_file &lt;- st_transform(NYC_CC_file, crs = \"WGS84\")\n  \n  NYC_CC_file &lt;- NYC_CC_file |&gt;\n    mutate(geometry = st_simplify(geometry, dTolerance = 10))\n  \n  return(NYC_CC_file)\n}\n\nnyc_council &lt;- NYC_CC_2025()\n\n\n\n\nNYC Tree Points\nThen, we need to dowload NYC Tree Points to locate the trees geographically and other crucial information.\n\n\n\n\n\n\nNoteTask 2: Download Tree Points\n\n\n\n\n\n\n\n\nCode to download data\nAll_Trees_GeoJSON &lt;- function(\n  endpoint = \"https://data.cityofnewyork.us/resource/hn5i-inap.geojson\",\n  limit = 10000,         \n  save_dir = \"data/mp03\",\n  max_pages = 100        \n) {\n  if (!dir.exists(save_dir)) dir.create(save_dir, recursive = TRUE)\n  \n  all_batches &lt;- list()\n  offset &lt;- 0\n  page &lt;- 1\n  \n  cat(\"Downloading (or reading cached) NYC Tree GeoJSON batches...\\n\")\n  pb &lt;- txtProgressBar(min = 0, max = max_pages, style = 3)\n  \n  repeat {\n    file_path &lt;- file.path(save_dir, paste0(\"trees_batch_\", offset, \".geojson\"))\n    \n    if (!file.exists(file_path)) {\n      cat(\"\\n Downloading batch \", page, \" (offset=\", offset, \")...\\n\", sep = \"\")\n      req &lt;- request(endpoint) %&gt;%\n        req_url_query(`$limit` = limit, `$offset` = offset)\n      req_perform(req, path = file_path)\n      Sys.sleep(1)\n    } else {\n      cat(\"\\n✓ Using cached batch \", page, \" (offset=\", offset, \")\\n\", sep = \"\")\n    }\n    \n    batch_data &lt;- tryCatch(\n      st_read(file_path, quiet = TRUE),\n      error = function(e) {\n        cat(\"Error reading batch at offset\", offset, \"\\n\")\n        return(NULL)\n      }\n    )\n    \n    if (!is.null(batch_data) && \"planteddate\" %in% names(batch_data)) {\n      batch_data$planteddate &lt;- as.character(batch_data$planteddate)\n    }\n\n    if (is.null(batch_data) || nrow(batch_data) == 0) break\n   \n    all_batches[[page]] &lt;- batch_data\n    setTxtProgressBar(pb, page)\n    if (nrow(batch_data) &lt; limit) {\n      cat(\"\\n✓ Last batch received (\", nrow(batch_data), \" rows)\\n\", sep = \"\")\n      break\n    }\n    offset &lt;- offset + limit\n    page &lt;- page + 1\n    if (page &gt; max_pages) {\n      warning(\"\\nReached max_pages limit; stopping to prevent infinite loop.\")\n      break\n    }\n  }\n  \n  close(pb)\n  cat(\"\\n Combining all downloaded batches...\\n\")\n  full_data &lt;- do.call(bind_rows, all_batches)\n  trees_sf &lt;- st_transform(full_data, crs = 4326)\n  \n  cat(\"Finished! Total trees: \", nrow(trees_sf), \"\\n\", sep = \"\")\n  return(trees_sf)\n}\n\ntree_all &lt;- All_Trees_GeoJSON(\n  endpoint = \"https://data.cityofnewyork.us/resource/hn5i-inap.geojson\",\n  limit = 10000,\n  save_dir = \"data/mp03\"\n)\n\n\n\n\n\n\nData Integration\nNow that we have downloaded our datasets, we will be analyzing the data to answer several questions to help us understand the data se have obtained.\n\nMapping NYC Trees\n\n\n\n\n\n\nNoteTask 3: Plot All Tree Points\n\n\n\n\n\n\nThe graph below reveals all trees within NYC, with a gradient noting the number of trees in different councils.\n\n\nCode\nlibrary(ggplot2)\n\ntree_with_district &lt;- st_join(tree_all, nyc_council, join = st_within, left = FALSE)\n\ntree_count_by_district &lt;- tree_with_district |&gt;\n  group_by(CounDist) |&gt; \n  summarise(num_trees = n()) |&gt;\n  arrange(desc(num_trees))\n\ntree_count_by_district_df &lt;- tree_count_by_district |&gt; \n  st_drop_geometry()\n\nnyc_council_choro &lt;- nyc_council |&gt;\n  left_join(tree_count_by_district_df, by = \"CounDist\")\n\nggplot(nyc_council_choro) +\n  geom_sf(aes(fill = num_trees), color = \"white\", size = 0.3) +\n  scale_fill_gradient(low = \"#e5f5e0\",high = \"#006d2c\", trans = \"sqrt\") +\n  labs(\n    title = \"Tree Counts by NYC City Council District\",\n    subtitle = \"NYC OpenData Tree Inventory\",\n    fill = \"Number of Trees\",\n    caption = \"Data: NYC OpenData\"\n  ) +\n  theme_minimal(base_size = 12) +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\n\nThis interactive tree map allows us to the exact location of trees while being able to directly choose which district we want to explore.\n\n\nCode\nlibrary(leaflet)\n\ntree_sample &lt;- tree_all |&gt; slice_sample(n = 2000)\n\nleaflet(tree_sample) |&gt;\n  addTiles() |&gt;\n  addCircleMarkers(\n    radius = 2,\n    color = \"green\",\n    stroke = FALSE,\n    fillOpacity = 0.5,\n    clusterOptions = markerClusterOptions() \n  ) |&gt;\n  setView(lng = -73.98, lat = 40.73, zoom = 11)\n\n\n\n\n\n\n\n\nDistrict-Level Analyses of Trees\n\n\n\n\n\n\nNoteTask 4: District-Level Analysis of Tree Coverage\n\n\n\n\n\n\n1. Which council district has the most trees?\n\n\nCode\nmost_trees &lt;- tree_count_by_district[1,]\nmost_trees_dis &lt;- most_trees$CounDist\nmost_trees_amount &lt;- most_trees$num_trees\n\n\nCouncil district 51 has the most trees across all districts, with total of 66,708 trees.\n2. Which council district has the highest density of trees? The Shape_Area column from the district shape file will be helpful here.\n\n\nCode\ntree_density &lt;- tree_all |&gt;\n  st_join(nyc_council, join = st_intersects) |&gt;\n  st_drop_geometry() |&gt;\n  count(CounDist, name = \"n_trees\") |&gt;\n  left_join(\n    nyc_council |&gt;\n      mutate(Area_calc = as.numeric(st_area(geometry))) |&gt;\n      st_drop_geometry() |&gt;\n      select(CounDist, Area_calc),\n    by = \"CounDist\"\n  ) |&gt;\n  mutate(tree_density = n_trees / Area_calc) |&gt;\n  arrange(desc(tree_density))\n\ntree_density |&gt; slice(1)\n\n\n  CounDist n_trees Area_calc tree_density\n1       39   30777  10980423  0.002802897\n\n\nCode\nhighest_density_council &lt;- tree_density |&gt; slice(1)\nhighest_den_dis &lt;- highest_density_council$CounDist\nhigh_den_per &lt;- highest_density_council$tree_density\n\n\nThe council with the highest denisty of trees was district 39, with a density percentage of 0.3%.\n3. Which district has highest fraction of dead trees out of all trees?\n\n\nCode\ndead_fraction &lt;- tree_with_district |&gt;\n  st_drop_geometry() |&gt;\n  group_by(CounDist) |&gt;\n  summarise(\n    total_trees = n(),\n    dead_trees = sum(tpcondition == \"Dead\", na.rm = TRUE),\n    dead_fraction = dead_trees / total_trees\n  ) |&gt;\n  arrange(desc(dead_fraction))\n\nhighest_dead_fraction &lt;- dead_fraction |&gt; slice(1)\ndead_district &lt;- highest_dead_fraction$CounDist\ndead_per &lt;- highest_dead_fraction$dead_fraction\n\n\nThrough our findings, it is clear that district 32 has the highest fraction of dead trees out of all trees, with a percentage of 15%.\n4. What is the most common tree species in Manhattan?\n\n\nCode\nlibrary(dplyr)\n\ntree_with_borough &lt;- tree_with_district |&gt;\n  mutate(\n    borough = case_when(\n      CounDist &gt;= 1 & CounDist &lt;= 10 ~ \"Manhattan\", \n      CounDist &gt;= 11 & CounDist &lt;= 18 ~ \"Bronx\",\n      CounDist &gt;= 19 & CounDist &lt;= 32 ~ \"Queens\", \n      CounDist &gt;= 33 & CounDist &lt;= 48 ~ \"Brooklyn\", \n      CounDist &gt;= 49 & CounDist &lt;= 51 ~ \"Staten Island\", \n      TRUE ~ NA_character_\n    )\n  )\n\nmost_popular_mnhtn_species &lt;- tree_with_borough |&gt;\n  st_drop_geometry() |&gt;\n  filter(borough == \"Manhattan\") |&gt;\n  count(genusspecies, sort = TRUE) |&gt;\n  slice(1)\n\nmanhattan_tree_name &lt;- most_popular_mnhtn_species$genusspecies\nmanhattan_num &lt;- most_popular_mnhtn_species$n\n\n\nManhattan has a great variety of tree species, however, the most common was Gleditsia triacanthos var. inermis - Thornless honeylocust, with 16,846 trees in total.\n5. What is the species of the tree closest to Baruch’s campus?\n\n\nCode\nnew_st_point &lt;- function(lat, lon, ...){\n    st_sfc(point = st_point(c(lon, lat))) |&gt;\n      st_set_crs(\"WGS84\")\n}\n\nBaruch_point &lt;- new_st_point(lat = 40.74045, lon = -73.98322)\n\nclosest_tree_baruch &lt;- tree_all |&gt;\n  mutate(distance = st_distance(geometry, Baruch_point)) |&gt;\n  arrange(distance) |&gt;\n  slice(1)\n\n\nclosest_coords &lt;- st_coordinates(closest_tree_baruch$geometry)\n\nbaruch_longitude &lt;- -73.98322\nbaruch_latitude  &lt;- 40.74045\n\nleaflet() |&gt;\n  addTiles() |&gt;\n  setView(baruch_longitude, baruch_latitude, zoom = 17) |&gt;\n  \n  addMarkers(baruch_longitude, baruch_latitude, popup = \"Baruch College\") |&gt;\n  \n  addMarkers(closest_coords[1], closest_coords[2],\n             popup = paste0(\"Closest tree: \", closest_tree_baruch$genusspecies)) \n\n\n\n\n\n\nCode\nbaruch_species &lt;- closest_tree_baruch$genusspecies\n\n\nThe closest tree near Baruch College is a Pyrus calleryana - Callery pear.\n\n\n\n\nGovernment Project Design\n\n\n\n\n\n\nNoteTask 5: NYC Parks Proposal\n\n\n\n\n\n\n\nProposal to NYC Park’s Department\nAs a NYC council member, I would like to request the NYC Park Department additional funding for our next project dealing with the improvement of trees in critical condition. These trees are extremely close to the end of their life, therefore we hope to identify their underlying issue to restore them to their best condition.\nRather than focusing solely on the district with the highest amount of trees in critical condition, working on trees based on critical fraction is our goal. We will act with urgency to tackle the district with the highest percentage and then share information about our approach to other districts with high numbers as well.\n\nQuantitative Data\n\n\nCode\ncritical_condition_trees &lt;- tree_with_district |&gt;\n  group_by(CounDist) |&gt; \n  summarise(\n    total_trees = n(),\n    critical_trees = sum(tpcondition == \"Critical\", na.rm = TRUE),\n    critical_fraction = critical_trees / total_trees\n  ) |&gt;\n  arrange(desc(critical_fraction)) |&gt; \n  slice(1)\n\ntotal_trees_dis10 &lt;- critical_condition_trees$total_trees\ndis10_critical_trees &lt;- critical_condition_trees$critical_trees \n\n\nBased on our data, we found that District 10 has the highest critical condition percentage among all districts.\n\nTotal Trees = 13963\nCritical Trees = 182\n$200,000 cost to restore all District 10 critical trees with intensive treatment\n\n\n\nCode\ndistrict_10 &lt;- nyc_council |&gt; filter(CounDist == 10)\n\ntrees_district_10 &lt;- tree_with_district |&gt;\n  filter(CounDist == 10) |&gt;\n  mutate(\n    condition_status = case_when(\n      tpcondition == \"Good\" ~ \"Good\",\n      tpcondition == \"Fair\" ~ \"Fair\",\n      tpcondition == \"Poor\" ~ \"Poor\",\n      tpcondition == \"Critical\" ~ \"Critical\",\n      TRUE ~ \"Unknown\"\n    )\n  )\n\ntree_sample &lt;- trees_district_10 |&gt; slice_sample(n = min(nrow(trees_district_10), 13963))\n\npal &lt;- colorFactor(\n  palette = c(\"Critical\" = \"red\", \"Good\" = \"green\",  \"Fair\" = \"yellow\", \"Poor\" = \"orange\", \"Unknown\" = \"gray\"),\n  domain = tree_sample$condition_status\n)\n\nleaflet() |&gt;\n  addTiles() |&gt;\n  addPolygons(\n    data = district_10,\n    fillColor = \"transparent\",\n    color = \"black\",\n    weight = 2,\n    popup = ~paste(\"District:\", CounDist)\n  ) |&gt;\n  addCircleMarkers(\n    data = tree_sample,\n    radius = 3,\n    color = ~pal(condition_status),\n    stroke = FALSE,\n    fillOpacity = 0.7,\n    clusterOptions = markerClusterOptions(),\n    popup = ~paste(\"Species:\", genusspecies, \"&lt;br&gt;Condition:\", tpcondition)\n  ) |&gt;\n  setView(\n    lng = st_coordinates(st_centroid(district_10))[1],\n    lat = st_coordinates(st_centroid(district_10))[2],\n    zoom = 15\n  ) |&gt;\n  addLegend(\n    \"bottomright\", \n    pal = pal, \n    values = tree_sample$condition_status,\n    title = \"Tree Condition in District 10\"\n  )\n\n\n\n\n\n\n\n\nWhy Is District 10 The Right Place?\nAlthough there is a total of fewer trees compared to other districts, the critical fraction is relatively high, signaling that its tree population may be in danger.\n\n\nCode\ncritical_condition_comparison &lt;- tree_with_district |&gt;\n  group_by(CounDist) |&gt; \n  summarise(\n    total_trees = n(),\n    critical_trees = sum(tpcondition == \"Critical\", na.rm = TRUE),\n    critical_fraction = critical_trees / total_trees\n  ) |&gt;\n  arrange(desc(critical_fraction)) |&gt; \n  slice_head(n=5) \n\nlibrary(DT)\nlibrary(stringr)\n\nformat_titles &lt;- function(df){\n  colnames(df) &lt;- str_replace_all(colnames(df), \"_\", \" \") |&gt; str_to_title()\n  df\n}\n\ncritical_condition_comparison |&gt;\n  select(-geometry) |&gt;\n  format_titles() |&gt;\n  datatable(\n    options = list(\n      searching = FALSE, \n      info = FALSE, \n      pageLength = 5,\n      columnDefs = list(list(className = 'dt-center', targets = \"_all\"))\n    ),\n    rownames = FALSE\n  ) |&gt;\n  formatPercentage(\"Critical Fraction\", 1) |&gt;\n  formatStyle(\n    \"Coundist\", \n    target = 'row',\n    backgroundColor = styleEqual(\"10\", \"lightblue\"), \n    fontWeight = styleEqual(\"10\", \"bold\")\n  ) |&gt;\n  formatRound(c(\"Total Trees\", \"Critical Trees\"), 0) \n\n\n\n\n\n\n\n\nCode\nggplot(critical_condition_comparison, aes(x = reorder(CounDist, critical_fraction), y = critical_fraction*100)) +\n  geom_col(fill = \"red\") +\n  coord_flip() + \n  labs(\n    title = \"Top 5 NYC City Council Districts by Fraction of Critical Trees\",\n    x = \"Council District\",\n    y = \"Fraction of Trees in Critical Condition\",\n    caption = \"Data: NYC OpenData\"\n  ) +\n  theme_minimal(base_size = 8) +\n  geom_text(aes(label = paste0(round(critical_fraction*100,1), \"%\")), \n            hjust = -0.1, size = 5) +\n  scale_y_continuous(limits = c(0, max(critical_condition_comparison$critical_fraction*100)*1.2))\n\n\n\n\n\n\n\n\n\nCompared to District 9, District 10 shows a higher density of trees in critical condition, which is what this project hopes to target. We can see where the concentration truly is and which areas we should target first in case its causing any problems.\n\n\nCode\nlibrary(patchwork)  \n\ndistricts_9_10 &lt;- nyc_council |&gt; filter(CounDist %in% c(9, 10))\n\ntrees_9_10 &lt;- tree_with_district |&gt;\n  filter(CounDist %in% c(9, 10)) |&gt;\n  mutate(critical_flag = ifelse(tpcondition == \"Critical\", \"Critical\", \"Other\"))\n\nplot_heatmap &lt;- function(district_number){\n  district &lt;- districts_9_10 |&gt; filter(CounDist == district_number)\n  trees &lt;- trees_9_10 |&gt; filter(CounDist == district_number, critical_flag == \"Critical\") |&gt;\n    mutate(lon = st_coordinates(geometry)[,1],\n           lat = st_coordinates(geometry)[,2])\n  \n  ggplot() +\n    geom_sf(data = district, fill = NA, color = \"black\", size = 0.5) +\n    stat_density_2d(\n      data = trees,\n      aes(x = lon, y = lat, fill = ..level.., alpha = ..level..),\n      geom = \"polygon\",\n      contour = TRUE\n    ) +\n    scale_fill_gradient(low = \"green\", high = \"red\") +\n    scale_alpha(range = c(0.1, 0.5), guide = \"none\") +\n    labs(\n      title = NULL,\n      fill = \"Density\"\n    ) +\n    theme_minimal(base_size = 12) +\n    theme(\n      axis.title = element_blank(),\n      axis.text = element_blank(),\n      axis.ticks = element_blank()\n    )\n}\n\np9 &lt;- plot_heatmap(9) + theme(legend.position = \"none\")\np10 &lt;- plot_heatmap(10)\n\ncombined_plot &lt;- p9 + p10 +\n  plot_annotation(\n    title = \"Highest Concentration of Critical Trees\",\n    caption = \"Left: District 9 | Right: District 10\"\n  )\n\ncombined_plot\n\n\n\n\n\n\n\n\n\nTo conclude, District 10 needs immediate assistance to treat these trees to improve everything and everyone’s quality of life. This project will not only protect those trees for the future, but protect everyone from its dangers in the present."
  },
  {
    "objectID": "mp03.html#task-1-download-nyc-city-council-district-boundaries",
    "href": "mp03.html#task-1-download-nyc-city-council-district-boundaries",
    "title": "Visualizing and Maintaining the Green Canopy of NYC",
    "section": "Task 1: Download NYC CIty Council District Boundaries",
    "text": "Task 1: Download NYC CIty Council District Boundaries"
  },
  {
    "objectID": "mp03.html#task-2-download-tree-points",
    "href": "mp03.html#task-2-download-tree-points",
    "title": "Visualizing and Maintaining the Green Canopy of NYC",
    "section": "Task 2: Download Tree Points",
    "text": "Task 2: Download Tree Points"
  },
  {
    "objectID": "mp03.html#mapping-nyc-trees",
    "href": "mp03.html#mapping-nyc-trees",
    "title": "Visualizing and Maintaining the Green Canopy of NYC",
    "section": "Mapping NYC Trees",
    "text": "Mapping NYC Trees\n\nTask 3: Plot All Tree Points"
  },
  {
    "objectID": "mp03.html#district-level-analyses-of-trees",
    "href": "mp03.html#district-level-analyses-of-trees",
    "title": "Visualizing and Maintaining the Green Canopy of NYC",
    "section": "District-Level Analyses of Trees",
    "text": "District-Level Analyses of Trees\n\nTask 4: District-Level Analysis of Tree Coverage\n1. Which council district has the most trees?\n2. Which council district has the highest density of trees? The Shape_Area column from the district shape file will be helpful here.\n3. Which district has highest fraction of dead trees out of all trees?\n4. What is the most common tree species in Manhattan?\n5. What is the species of the tree closest to Baruch’s campus?"
  },
  {
    "objectID": "mp04.html",
    "href": "mp04.html",
    "title": "Just the Fact(-Check)s, Ma’am!",
    "section": "",
    "text": "Throughout this report, we will obtain and analyze data to evaluate President Trump’s rationale in firing the Commissioner of Labor Statistics, Dr. Erika McEntarfer."
  },
  {
    "objectID": "mp04.html#data-acquisition",
    "href": "mp04.html#data-acquisition",
    "title": "Just the Fact(-Check)s, Ma’am!",
    "section": "",
    "text": "Your analysis will depend on two separate data sources:\nThe (final) CES estimates of the total employment level of the United States; and The cycle-to-cycle revisions of the CES estimate. #have to reword this\n\n\n\n\n\n\nNoteTask 1: Download CES Total Nonfarm Payroll\n\n\n\n\n\n\n\n\n\n\n\n\nNoteTask 2: Download CES Revisions Tables"
  },
  {
    "objectID": "mp04.html#data-integration",
    "href": "mp04.html#data-integration",
    "title": "Just the Fact(-Check)s, Ma’am!",
    "section": "",
    "text": "some writing here"
  },
  {
    "objectID": "mp04.html#data-integration-and-exploration",
    "href": "mp04.html#data-integration-and-exploration",
    "title": "Just the Fact(-Check)s, Ma’am!",
    "section": "",
    "text": "NoteTask 3: Data Exploration and Visualization"
  },
  {
    "objectID": "mp04.html#statistics",
    "href": "mp04.html#statistics",
    "title": "Just the Fact(-Check)s, Ma’am!",
    "section": "Statistics",
    "text": "Statistics\n\nLargest Possible Revision\n\n\nCode\nl_ps_rev &lt;- ces_joined |&gt; \n  filter(revision == max(revision, na.rm = TRUE)) |&gt;\n  select(date, original, final, revision, month_year)\n\nces_joined|&gt; \n  arrange(desc(revision)) |&gt; \n  slice_head(n = 5) |&gt; \n  format_titles() |&gt; \n  datatable(\n    caption = \"Largest 5 Positive CES Revisions\",\n    options = list(searching=FALSE, info=FALSE)\n  )\n\n\n\n\n\n\nThis helps us identify the single month in which initial employment understated the final estimate by the largest estimate. This is useful to understand extreme cases of first-estimate error and explore what economic conditions or data issues led to such a large upward revision.\nBased on our findings, we were able to see the largest upward revision occurred in Nov 2021, with an increase of 437,000 jobs.\n\n\nLargest Negative Revision\n\n\nCode\nl_negative_rev &lt;- ces_joined |&gt; \n  filter(revision == min(revision, na.rm = TRUE)) |&gt;\n  select(date, original, final, revision, month_year)\n\nces_joined|&gt; \n  arrange(revision) |&gt; \n  slice_head(n = 5) |&gt; \n  format_titles() |&gt; \n  datatable(\n    caption = \"Largest 5 Negative CES Revisions\",\n    options = list(searching=FALSE, info=FALSE)\n  )\n\n\n\n\n\n\nThis statistic will allow us to find the single month in which initial employment employment overstated the final estimate by the largest estimate, representing a downward correction.\nThrough our analysis, we were able to find out the largest negative revision occurred in Mar 2020. There was a total decrease in jobs of -672,000 this month for all 45 years.\n\n\nMean Absolute Revision\n\n\nCode\nMean_abs_rev &lt;-ces_joined |&gt;\n  summarise(mean_abs_revision = mean(abs(revision), na.rm = TRUE))\n\n\nBased on our findings, the mean absolute revision of 56.8960573 thousand jobs indicates that the preliminary CES payroll estimate typically differs from the final bench marked estimate by about 57,000 jobs each month.\n\n\nFraction Positive by Year Or Decade (CES Revisions)\n\n\nCode\nces_pr_rev2 &lt;- ces_joined |&gt; \n  mutate(\n    year = year(date), \n    decade = floor(year/10) * 10, \n    month = month(date, label = TRUE, abbr = TRUE)\n  )\n\n\nThis statistic allows us to compute the fraction of positive revisions to tells us whether BLS revisions tend to be biased upward or downward in a given period. Additionally, it demonstrates patterns by decade, which in turn reveal long-run structural differences in the labor market.\n\n\nAbsolute Revision as Percentage of the Monthly Payroll Level\n\n\nCode\nces_pr_rev_pctlevel &lt;- ces_pr_rev2 |&gt;\n  mutate(\n    abs_rev_pct_level= abs(revision) / ifelse(level == 0, NA_real_, level)\n    )\n\nyearly_absrev_pctlevel &lt;- ces_pr_rev_pctlevel |&gt;\n  group_by(year) |&gt;\n  summarise(\n    avg_absrev_pct = mean(abs_rev_pct_level, na.rm = TRUE), \n    med_absrev_pct = median(abs_rev_pct_level, na.rm = TRUE),\n    n = n()) |&gt;\n  arrange(year)\n\n\nThe purpose and function of this computation estimates the revision size relative to the total employment level in the given month. Essentially, it helps us understand whether revisions are becoming more or less important relative to the size of the labor market.\n\n\nRevision Magnitude by Calendar Month\n\n\nCode\nmonth_summary &lt;- ces_pr_rev2 |&gt;\n  group_by(month) |&gt; \n  summarise(\n    avg_rev = mean(revision, na.rm = TRUE),\n    mean_abs_revision = mean(abs(revision), na.rm = TRUE), \n    med_abs_rev = median(abs(revision), na.rm = TRUE),\n    n = n()) |&gt;\n  arrange(month)\n\n\nFinally, this analysis identifies whether certain months are consistently producing larger or smaller revisions."
  },
  {
    "objectID": "mp04.html#visualizations",
    "href": "mp04.html#visualizations",
    "title": "Just the Fact(-Check)s, Ma’am!",
    "section": "Visualizations",
    "text": "Visualizations\n\n\nCode\nggplot(ces_joined, aes(date, revision)) +\n  geom_col(fill = \"gray80\") +\n  geom_point(data = l_ps_rev, aes(date, revision), color = \"blue\", size = 4) +\n  geom_point(data = l_negative_rev, aes(date, revision), color = \"red\", size = 4) +\n  geom_text(\n    data = l_ps_rev,\n    aes(label = paste0(\"Largest +: \", revision)),\n    nudge_y = 200, size = 3, color = \"blue\"\n  ) +\n  geom_text(\n    data = l_negative_rev,\n    aes(label = paste0(\"Largest -: \", revision)),\n    nudge_y = -200, size = 3, color = \"red\"\n  ) +\n  labs(\n    title = \"Largest Positive & Negative CES Revisions (1979–2025)\",\n    x = \"Date\", \n    y = \"Revision\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nCode\nfraction_positive_by_year &lt;- ces_pr_rev2 |&gt; \n  group_by(year) |&gt; \n  summarise(\n    frac_pos = mean(revision &gt; 0, na.rm = TRUE)\n  )\n\nggplot(fraction_positive_by_year, aes(year, frac_pos)) +\n  geom_line() +\n  geom_point() +\n  scale_y_continuous(labels = scales::percent_format()) +\n  labs(\n    title = \"Fraction of Positive CES Revisions – By Year\",\n    x = \"Year\", \n    y = \"% Positive Revisions\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(yearly_absrev_pctlevel, aes(year, avg_absrev_pct)) +\n  geom_line() +\n  geom_point() +\n  geom_smooth(se = FALSE, linetype = \"dashed\") +\n  scale_y_continuous(labels = scales::percent_format()) +\n  labs(\n    title = \"Average Absolute Revision as % of Employment Level\",\n    x = \"Year\", \n    y = \"Average Absolute Revision / Level\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(ces_pr_rev2, aes(month, abs(revision))) +\n  geom_boxplot(fill = \"steelblue\", alpha = 0.7) +\n  scale_y_continuous(labels = scales::comma_format()) +\n  labs(\n    title = \"Distribution of Absolute CES Revisions by Calendar Month\",\n    x = \"Month\",\n    y = \"Absolute Revision\"\n  ) +\n  theme_minimal()"
  },
  {
    "objectID": "ind_rep.html#literature-and-background",
    "href": "ind_rep.html#literature-and-background",
    "title": "Citi Bike Usage in NYC: Individual Report",
    "section": "Literature and Background",
    "text": "Literature and Background\nBased on previous research on bike-share patterns and ridership, observations are consistent, including weekday commuting peaks and season variation. Multiple studies of Citi Bike data indicate that time-of-day and day-of-week explain a larger share of ridership variation compared to other individual rider characteristics."
  },
  {
    "objectID": "ind_rep.html#data-and-methods",
    "href": "ind_rep.html#data-and-methods",
    "title": "Citi Bike Usage in NYC: Individual Report",
    "section": "Data and Methods",
    "text": "Data and Methods\n\n3.1 Data Sources\nCiti Bike Trip Data\nThis study uses monthly trip-level data from October 2024 through October 2025. The dataset provides detailed records including the trip’s start time, station information, geographic coordinates, and rider classification (member or casual). It offers high temporal resolution and comprehensive coverage of bikeshare activity across New York City, making them well suited for analyzing short-term incurring temporal patterns in ridership.\nThe analysis of the specific question focuses specifically on trip start times, as this best reflects rider decision-making and demand for bikeshare services. Using such trip-level data allows for more flexible aggregation across varying temporal scales while preserving the underlying structure of usage patterns.\n\n\n3.2 Data Processing\n\n\nCode\nlibrary(data.table)\nlibrary(ggplot2)\nlibrary(scales)\nlibrary(lubridate)\n\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:data.table':\n\n    hour, isoweek, mday, minute, month, quarter, second, wday, week,\n    yday, year\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\n\nCode\nlibrary(ggtext)\nlibrary(broom)\n\ncitibike &lt;- readRDS(\n  \"data/processed/citibike_manhattan_all_202410_202510.rds\"\n)\n\n# REMOVE September 2024 completely\ncitibike &lt;- citibike[format(date, \"%Y-%m\") != \"2024-09\"]\n\n# Re-create time variables AFTER filtering\ncitibike[, month := format(date, \"%Y-%m\")]\ncitibike[, dow := wday(date, label = TRUE, week_start = 1)]\ncitibike[, wknd := ifelse(dow %in% c(\"Sat\", \"Sun\"), \"Weekend\", \"Weekday\")]\n\n\nSeveral processing steps were applied to prepare the Citi Bike data for analysis. First, trips were filtered to only include rides starting in Manhattan. By using spatial containment with borough boundary shape-files, it ensured that only trips with starting coordinated falling within the Manhattan borough polygon were retained. This is a crucial step to focus on Manhattan and remove any trips originating in other boroughs.\nTo further restrict the defined study period, September 2024 had to be manually removed. Although the study period starts on October 2024, September 2024 appeared as a spillover record due late-night and delayed trips included in October 2024 Citi Bike release. Since the month was incomplete, it was excluded in order to maintain temporal consistency.  \nFrom the cleaned data, multiple temporal variables were created:\n\nHour of Day (hour)\nMonth (month)\nDay of week (dow)\nWeekend indicator (wknd)\nCentered hour (hour_c)\nHour fixed effects (factor(hour)) \n\nAggregating trips to hourly and daily counts, reducing noise inherent individual trip records, and enabling clearer visualization and interpretable temporal modeling. These steps were necessary and appropriate for identifying systematic patterns in bike-share usage, while also retaining sufficient temporal detail for meaningful analysis.\n\n\n3.3 Analytic Approach\n\n\nCode\nhour_wknd &lt;- citibike[\n  ,\n  .(N = .N),\n  by = .(month, hour, wknd)\n]\n\nggplot(hour_wknd, aes(x = hour, y = N, color = wknd)) +\n  geom_line(linewidth = 1) +\n  facet_wrap(~ month) +\n  labs(\n    title = \"**Hourly Ridership Patterns: Weekday vs Weekend**\",\n    x = \"Hour\",\n    y = \"Trips\",\n    color = \"\",\n    caption = \"Data from Citi Bike (Manhattan starts only)\"\n  ) +\n  scale_y_continuous(labels = comma) +\n  scale_x_continuous(breaks = seq(0, 23, by = 3)) +\n  theme_minimal() +\n  theme(\n    plot.title = element_markdown(),\n    legend.position = \"top\",\n    panel.spacing = unit(1, \"lines\")\n  )\n\n\n\n\n\n\n\n\n\nFigure 1. Weekdays vs. Weekend Patterns.\nHourly ridership was compared between weekdays and weekends to distinguish commute-oriented usage from leisure-oriented travel, revealing weekday peaks and flatter weekend trends.\n\n\nCode\nheatmap_dt &lt;- citibike[\n  ,\n  .(N = .N),\n  by = .(dow, hour)\n]\n\nggplot(heatmap_dt, aes(x = hour, y = dow, fill = N)) +\n  geom_tile() +\n  labs(\n    title = \"**Hourly Ridership by Day of Week**\",\n    x = \"Hour of Day\",\n    y = \"\",\n    fill = \"Trips\"\n  ) +\n  scale_fill_viridis_c(labels = comma) +\n  scale_x_continuous(breaks = seq(0, 23, 3)) +\n  theme_minimal() +\n  theme(plot.title = element_markdown())\n\n\n\n\n\n\n\n\n\nFigure 2. Hour x Day-of-Week Heat Map.\nThis heat map was used to visualize hourly and weekly ridership intensity for Manhattan. Weekends display strong daytime and commute peaks, while weekends exhibit flatter, more evenly distributed ridership patterns consistent with non-commuting travel.\n\n\nCode\nlibrary(broom)\nhourly_counts &lt;- citibike[\n  , .N, by = .(date, hour, wknd)\n]\n\nhourly_counts[, hour_c := hour - mean(hour)]\n\nmodel &lt;- lm(N ~ factor(hour) + wknd, data = hourly_counts)\n\n\ncoef_df &lt;- broom::tidy(model, conf.int = TRUE)\ncoef_df$hour_num &lt;- as.numeric(gsub(\"factor\\\\(hour\\\\)\", \"\", coef_df$term))\ncoef_df &lt;- coef_df[order(coef_df$hour_num, decreasing = TRUE), ]\ncoef_df &lt;- coef_df[coef_df$term != \"(Intercept)\", ]\n\ncoef_df$term &lt;- ifelse(\n  coef_df$term == \"wkndWeekend\",\n  \"Weekend (vs Weekday)\",\n  paste0(\"Hour \", coef_df$hour_num)\n)\n\nggplot(coef_df, aes(x = estimate, y = term)) +\n  geom_point() +\n  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high), height = 0.2) +\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  labs(\n    title = \"**Regression Coefficients: Hour & Weekend Effects**\",\n    x = \"Estimate\",\n    y = \"\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_markdown())\n\n\n\n\n\n\n\n\n\nFigure 3. Regression Coefficients for Hour-of-day and Weekend Effects.\nCoefficient estimated from a linear regression of hourly Citi Bike trip counts in Manhattan, using hour-of-day fixed effects and a weekend indicator."
  },
  {
    "objectID": "ind_rep.html#results",
    "href": "ind_rep.html#results",
    "title": "Citi Bike Usage in NYC: Individual Report",
    "section": "Results",
    "text": "Results\nsee if this worked"
  },
  {
    "objectID": "ind_rep.html#discussion",
    "href": "ind_rep.html#discussion",
    "title": "Citi Bike Usage in NYC: Individual Report",
    "section": "Discussion",
    "text": "Discussion\nAcross cities, bike-share usage typically follows a stable temporal structure, with clear morning and evening peaks during weekdays reflecting commuter behavior, and flatter, midday peaks during weekends indicating leisure-oriented travel. These temporal cycles persist across seasons, suggesting that time-based variables provide a robust baseline for understating ridership variation."
  },
  {
    "objectID": "ind_rep.html#conclusion",
    "href": "ind_rep.html#conclusion",
    "title": "Citi Bike Usage in NYC: Individual Report",
    "section": "Conclusion",
    "text": "Conclusion"
  },
  {
    "objectID": "ind_rep.html",
    "href": "ind_rep.html",
    "title": "Citi Bike Usage in NYC: Individual Report",
    "section": "",
    "text": "Among the multiple systems of transportation, bike-share systems have become a central component of urban networks. Offering flexible, low-emission travel options for commuters and recreational users in New York City, Citi Bike has emerged as the largest bike-share system in the U.S., Manhattan accounting a large share of total ridership due to the boruough’s density, employment concentration, and extensive station coverage.\nOur study addresses the overarching question:\n“Among time, location, and other rider characteristics, which variables are the most important drivers of Citi Bike trip volume in Manhattan?”\nAs an initial step toward addressing the broader question, this analysis examines a related sub-question:\n“How do time of day and day of the week affect Citi Bike ridership?”\nTemporal factors are widely recognized as strong predictors of transportation demand, especially in commuted-focused settings. By isolating Manhattan and analyzing hourly and weekly patterns, this study aims to measure the magnitude and consistency of time-based influences, highlighting their significance compared to other potential determinants."
  },
  {
    "objectID": "ind_rep.html#introduction",
    "href": "ind_rep.html#introduction",
    "title": "Citi Bike Usage in NYC: Individual Report",
    "section": "",
    "text": "Among the multiple systems of transportation, bike-share systems have become a central component of urban networks. Offering flexible, low-emission travel options for commuters and recreational users in New York City, Citi Bike has emerged as the largest bike-share system in the U.S., Manhattan accounting a large share of total ridership due to the boruough’s density, employment concentration, and extensive station coverage.\nOur study addresses the overarching question:\n“Among time, location, and other rider characteristics, which variables are the most important drivers of Citi Bike trip volume in Manhattan?”\nAs an initial step toward addressing the broader question, this analysis examines a related sub-question:\n“How do time of day and day of the week affect Citi Bike ridership?”\nTemporal factors are widely recognized as strong predictors of transportation demand, especially in commuted-focused settings. By isolating Manhattan and analyzing hourly and weekly patterns, this study aims to measure the magnitude and consistency of time-based influences, highlighting their significance compared to other potential determinants."
  },
  {
    "objectID": "ind_rep.html#literatury-review",
    "href": "ind_rep.html#literatury-review",
    "title": "Citi Bike Usage in NYC: Individual Report",
    "section": "Literatury Review",
    "text": "Literatury Review\nBased on previous research on bike-share patterns and ridership, observations are consistent, including weekday commuting peaks and season variation. Multiple studies of Citi Bike data indicate that time-of-day and day-of-week explain a larger share of ridership variation compared to other individual rider characteristics.\nAnalyses using Citi Bike data similarly highlight such importance of temporal effects. The NYC Data Science project Inferential Analysis on Citi Bike Ridership applies statistical modeling techniques to identify key determinants of usage to explain a substantial portion of ridership variation. Although this work provides valuable system-wide insights, its overall approach may obscure meaningful spatial differences within NYC, particularly between boroughs with distinct travel behaviors and infrastructure characteristics.\nThe ScienceDirect study reinforces the conclusion about temporal variables. Their inferential analysis shows that the same variables emerge as significant predictors, even when controlling other factors. Once again, this highlights the importance of temporal regularity in bike share usage."
  },
  {
    "objectID": "ind_rep.html#literary-review",
    "href": "ind_rep.html#literary-review",
    "title": "Citi Bike Usage in NYC: Individual Report",
    "section": "Literary Review",
    "text": "Literary Review\nBased on previous research on bike-share patterns and ridership, observations are consistent, including weekday commuting peaks and season variation. Multiple studies of Citi Bike data indicate that time-of-day and day-of-week explain a larger share of ridership variation compared to other individual rider characteristics.\nAnalyses using Citi Bike data similarly highlight such importance of temporal effects. The NYC Data Science project Inferential Analysis on Citi Bike Ridership applies statistical modeling techniques to identify key determinants of usage to explain a substantial portion of ridership variation. Although this work provides valuable system-wide insights, its overall approach may obscure meaningful spatial differences within NYC, particularly between boroughs with distinct travel behaviors and infrastructure characteristics.\nThe ScienceDirect study reinforces the conclusion about temporal variables. Their inferential analysis shows that the same variables emerge as significant predictors, even when controlling other factors. Once again, this highlights the importance of temporal regularity in bike share usage."
  }
]