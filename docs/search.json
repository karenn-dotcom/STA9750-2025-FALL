[
  {
    "objectID": "mp01.html#exploratory-analysis-data",
    "href": "mp01.html#exploratory-analysis-data",
    "title": "Mini-Project #01: Gourmet Cheeseburgers Across the Globe",
    "section": "Exploratory Analysis Data",
    "text": "Exploratory Analysis Data\n1) How many different countries does Netflix operate in? (You can use the viewing history as a proxy for countries in which Netflix operates.)\n\n\nCode\nCountries_total &lt;- COUNTRY_TOP_10 |&gt; \n  distinct(country_name) |&gt; \n  count()\n\n\nNetflix operates in 94 countries.\n2) Which non-English-language film has spent the most cumulative weeks in the global top 10? How many weeks did it spend?\n\n\nCode\nne_mcmtv_wks &lt;- GLOBAL_TOP_10 |&gt;\n  filter(category == \"Films (Non-English)\") |&gt;\n  slice_max(cumulative_weeks_in_top_10, n=1)\n\nnon_english_title &lt;- ne_mcmtv_wks$show_title\nnon_english_weeks &lt;- ne_mcmtv_wks$cumulative_weeks_in_top_10\n\n\nThe non-English movie that has spent the most cumulative weeks in the global Top 10 is All Quiet on the Western Front for a total of 23 weeks.\n3) What is the longest film (English or non-English) to have ever appeared in the Netflix global Top 10? How long is it in minutes?\n\n\nCode\nlongest_film &lt;- GLOBAL_TOP_10 |&gt; \n  filter(str_detect(category, \"Films\")) |&gt;\n  filter(!is.na(runtime)) |&gt;\n  mutate(runtime_minutes = round(60 * runtime)) |&gt;\n  arrange(desc(runtime_minutes)) |&gt;\n  slice(1)\n\nlongest_film_name &lt;- longest_film$show_title\nlongest_film_mins &lt;- longest_film$runtime_minutes\n\n\nThe longest film to ever appear the longest in global Top 10 is Pushpa 2: The Rule (Reloaded Version) for 224 minutes.\n4) For each of the four categories, what program has the most total hours of global viewership?\n\n\nShow Code\nmost_hours_prgms &lt;- GLOBAL_TOP_10 |&gt;\n  group_by(category, show_title) |&gt; \n  summarise(total_hours = sum(weekly_hours_viewed)) |&gt;\n  group_by(category) |&gt; \n  slice_max(total_hours, n=1) |&gt;\n  ungroup() |&gt;\n  arrange(desc(total_hours))\n\n\n\n\nCode\nmost_hours_prgms |&gt;\n  mutate(total_hours = scales::comma(total_hours)) |&gt;\n  format_titles() |&gt;\n  datatable(\n    most_hours_prgms, \n    options = list(searching = FALSE, info = FALSE),\n    caption = \"Top Programs' Total Hours by Category\")\n\n\n\n\n\n\n5) Which TV show had the longest run in a country’s Top 10? How long was this run and in what country did it occur?\n\n\nCode\nlongest_cn_run &lt;- COUNTRY_TOP_10 |&gt;\n  filter(str_detect(category, \"TV\")) |&gt;\n  group_by(country_name, show_title) |&gt;\n  summarise(mx_cmwks = max(cumulative_weeks_in_top_10, na.rm = TRUE), .groups = \"drop\") |&gt;\n  arrange(desc(mx_cmwks)) |&gt;\n  slice(1)\n\nlongest_cn_run_title &lt;- longest_cn_run$show_title\nlongest_cn_run_weeks &lt;- longest_cn_run$mx_cmwks\nlongest_cn_run_name &lt;- longest_cn_run$country_name\n\n\nMoney Heist had the longest run in a Pakistan’s Top 10 for a total of 127 weeks.\n6) Netflix provides over 200 weeks of service history for all but one country in our data set. Which country is this and when did Netflix cease operations in that country?\n\n\nCode\nna_200 &lt;- COUNTRY_TOP_10 |&gt;\n  group_by(country_name) |&gt;\n  summarise(total_least_weeks = n_distinct(week), last_wk = max(week, na.rm = TRUE), .groups = \"drop\") |&gt;\n  filter(total_least_weeks &lt; 200) |&gt;\n  arrange(total_least_weeks)\n\nleast_weeks_country &lt;- na_200$country_name\nleast_weeks_date &lt;- na_200$last_wk\n\n\nOn 2022-02-27, Netflix ceased their operations in Russia.\n7) What is the total viewership of the TV show Squid Game? Note that there are three seasons total and we are looking for the total number of hours watched across all seasons.\n\n\nCode\ntotal_viewership_sg &lt;- GLOBAL_TOP_10 |&gt;\n  filter(str_detect(show_title, \"Squid Game\")) |&gt;\n  summarise(total_hours_viewed = sum(weekly_hours_viewed, na.rm = TRUE)) |&gt;\n  pull(total_hours_viewed)\ntotal_viewership_squid_game &lt;- scales::comma(total_viewership_sg)\n\n\nThe total viewership of the TV show Squid Game is 5,310,000,000 hours, taking into account all three seasons.\n8) The movie Red Notice has a runtime of 1 hour and 58 minutes. Approximately how many views did it receive in 2021? Note that Netflix does not provide the weekly_views values that far back in the past, but you can compute it yourself using the total view time and the runtime.\n\n\nCode\nlibrary(lubridate)\nRed_Notice_21 &lt;- GLOBAL_TOP_10 |&gt;\n  filter(show_title == \"Red Notice\", year(week) == 2021) |&gt;\n  summarise(total_hours = sum(weekly_hours_viewed, na.rm = TRUE)) |&gt;\n  mutate(approx_views = total_hours / (1 + 58/60))\n\n\nThe total approximate views Red Notice received in 2021 is 201,732,203 views.\n9) How many Films reached Number 1 in the US but did not originally debut there? That is, find films that first appeared on the Top 10 chart at, e.g., Number 4 but then became more popular and eventually hit Number 1? What is the most recent film to pull this off?\n\n\nCode\ntop_US_films &lt;- COUNTRY_TOP_10 |&gt;\n  filter(country_name == \"United States\", str_detect(category, \"Films\")) |&gt;\n  arrange(week) |&gt;\n  group_by(show_title) |&gt;\n  summarise(\n    debut_week = min(week, na.rm = TRUE),\n    debut_rank = first(weekly_rank),\n    top_rank = min(weekly_rank, na.rm = TRUE),\n    latest_week = max(week)\n    ) |&gt;\n  filter(debut_rank &gt; 1, top_rank == 1)\n\nlastest_film &lt;- top_US_films |&gt; \n  arrange(desc(latest_week)) |&gt;\n  slice(1) \n\ntop_US_films |&gt;\n  format_titles() |&gt;\n  datatable(\n    options = list(searching= FALSE, info= FALSE), \n    caption = \"Films That Climed to Number 1\"\n  )\n\n\n\n\n\n\nCode\ntotal_top_films &lt;- nrow(top_US_films)\nrecent_top_1 &lt;- lastest_film$show_title\n\n\nThere is a total of 45 films that did not originally debut at number 1, but eventually ranked at the top. The most recent film to achieve this was KPop Demon Hunters.\n10) Which TV show/season hit the top 10 in the most countries in its debut week? In how many countries did it chart?\n\n\nCode\ntop_show_debut &lt;- COUNTRY_TOP_10 |&gt;\n  filter(category == \"TV\") |&gt;\n  group_by(show_title) |&gt;\n  filter(week == min(week)) |&gt; \n  summarise(countries_amount = n_distinct(country_name), .groups = \"drop\") |&gt;\n  arrange(desc(countries_amount)) |&gt; \n  slice(1)\n\nshow_name &lt;- top_show_debut$show_title\ncountries_charted &lt;- top_show_debut$countries_amount\n\n\nEmily in Paris hit the Top 10 in most countries in its debut week, with a total of 94 countries charted."
  },
  {
    "objectID": "mp01.html#the-rise-of-the-upside-down",
    "href": "mp01.html#the-rise-of-the-upside-down",
    "title": "Mini-Project #01: Gourmet Cheeseburgers Across the Globe",
    "section": "The Rise of the Upside Down",
    "text": "The Rise of the Upside Down\n\nPress Release 1: Upcoming Season of Stranger Things\n\n\n\nStranger Things Season 5\n\n\n\n\nCode\nlibrary(lubridate)\nst_views &lt;- GLOBAL_TOP_10 |&gt;\n  filter(show_title == \"Stranger Things\") |&gt;\n  summarise(total_hours = sum(weekly_hours_viewed, na.rm = TRUE))\n\nst_data &lt;- GLOBAL_TOP_10 %&gt;%\n  filter(str_detect(show_title, regex(\"Stranger Things\", ignore_case = TRUE))) %&gt;%\n  summarise(\n    total_weeks = max(cumulative_weeks_in_top_10, na.rm = TRUE)\n  )\n\ncompare_shows &lt;- GLOBAL_TOP_10 |&gt;\n  select(show_title, weekly_hours_viewed) |&gt; \n  filter(show_title == \"Stranger Things\" | \n           show_title == \"My Life With the Walter Boys\" | \n           show_title ==\"Love Is Blind: UK\" | \n           show_title == \"You\") |&gt; \n  group_by(show_title) |&gt; \n  summarise(total_weekly_hours = sum(weekly_hours_viewed))\n\nwalter_boys &lt;- compare_shows$show_title[2]\nyou_show &lt;- compare_shows$show_title[4]\nwalter_views &lt;- compare_shows$total_weekly_hours[2]\nyou_views &lt;- compare_shows$total_weekly_hours[4]\n\n\nAt last, Stranger Things is coming to an end, releasing its final season on November 26, 2025. Not only has it been a huge hit across the United States, but across other nations as well, garnering a total viewership of 2,967,980,000 hours. Compared to other popular shows such as You and My Life With the Walter Boys, they fell behind with 1,542,990,000 and 629,800,000 hours, respectively. These are astounding numbers, as we saw it was one of the top viewed shows and also ranked highly for several weeks. It has maintained itself 19 weeks in the Top 10.\nAs a Stranger Things watcher myself, I cannot wait to see if Hawkins will be consumed by the underworld, or if it will finally be closed for good."
  },
  {
    "objectID": "mp01.html#breaking-into-india-netflixs-ambitions-in-the-indian-market",
    "href": "mp01.html#breaking-into-india-netflixs-ambitions-in-the-indian-market",
    "title": "Mini-Project #01: Gourmet Cheeseburgers Across the Globe",
    "section": "Breaking Into India: Netflix’s Ambitions in the Indian Market",
    "text": "Breaking Into India: Netflix’s Ambitions in the Indian Market\n\nPress Release 2: Commercial Success in India\n\n\nCode\ntop_hindi_films &lt;- GLOBAL_TOP_10 |&gt; select(show_title, weekly_hours_viewed) |&gt; \n  filter(str_detect(show_title, \"Hindi\")) |&gt; \n  group_by(show_title) |&gt; \n  summarise(total_wkly_hrs = sum(weekly_hours_viewed)) |&gt;\n  slice_max(total_wkly_hrs, n = 5)\n  \nhindi_india &lt;- COUNTRY_TOP_10 %&gt;%\n  filter(country_name == \"India\") %&gt;%\n  filter(str_detect(show_title, regex(\"Hindi\", ignore_case = TRUE)) |\n         str_detect(season_title, regex(\"Hindi\", ignore_case = TRUE))) %&gt;%\n  group_by(show_title) %&gt;%\n  summarise(\n    weeks_top10 = max(cumulative_weeks_in_top_10, na.rm = TRUE),\n    peak_rank   = min(weekly_rank, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %&gt;%\n  arrange(peak_rank, desc(weeks_top10))\n\nrrr &lt;- top_hindi_films$show_title[1]\ntop_hindi_2 &lt;- top_hindi_films$show_title[2]\nrrr_hours &lt;-top_hindi_films$total_wkly_hrs[1]\ntop2_hours &lt;- top_hindi_films$total_wkly_hrs[2]\nrrr_rank_weeks &lt;- hindi_india$weeks_top10[1]\n\n\nBeing the most populated country in the world, India is an enticing market many companies are ready to enter and invest in, Netlfix being one of them. Currently RRR (Hindi) and Kalki 2898 AD (Hindi) sit at the top with 79,780,000 and 23,100,000 hours, respectively. Not to mention RRR (Hindi) has spent 25 weeks ranked 1. A total of 11 films have ranked that high, attracting Netflix even more."
  },
  {
    "objectID": "mp01.html#global-hits-non-english-tv-shows-taking-over-worldwide",
    "href": "mp01.html#global-hits-non-english-tv-shows-taking-over-worldwide",
    "title": "Mini-Project #01: Gourmet Cheeseburgers Across the Globe",
    "section": "Global Hits: Non-English TV Shows Taking Over Worldwide",
    "text": "Global Hits: Non-English TV Shows Taking Over Worldwide\n\nPress Release 3: Open Topic\n\n\nCode\nne__shows &lt;- GLOBAL_TOP_10 |&gt;\n  filter(category == \"TV (Non-English)\") |&gt;\n  group_by(show_title) |&gt;\n  summarise(\n    total_weeks = max(cumulative_weeks_in_top_10, na.rm = TRUE),\n    total_hours = sum(weekly_hours_viewed, na.rm = TRUE),\n    .groups = \"drop\"\n  ) |&gt;\n  arrange(desc(total_weeks))\n\nne__shows |&gt;\n  mutate(total_hours = scales::comma(total_hours)) |&gt;\n  head(10) |&gt;\n  format_titles() |&gt;\n  datatable(\n    options = list(searching= FALSE, info= FALSE), \n    caption = \"Top Non-English Shows\"\n  )\n\n\n\n\n\n\nCode\nbetty_show &lt;- ne__shows$show_title[2]\nbetty_wks &lt;- ne__shows$total_weeks[2]\nbetty_hours&lt;- ne__shows$total_hours[2]\n\n\nOver the last decade, we have seen an influx of foreign consumption worldwide. The table above demonstrates the TV shows’ amount of weeks spent in the Top 10. Aside from genre, there is also a variety of language in just these five shows, like Korean, Spanish, and Portuguese. Squid Game did extremly well, but Yo soy Betty, la fea did exceptional as well, with 30 weeks in the Top 10 and 297,560,000 hours in viewership. These Korean dramas and Spanish thrillers continue breaking barriers and dominating global charts."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My First Website",
    "section": "",
    "text": "My name is Karen Cruz and I am currently 22 years old. I am currently on my second semester doing the Master’s in Statistics program at CUNY Baruch.\nThere’s a multitude of things I enjoy doing during my free time such as trying new foods and restaurants, playing sports, traveling, and the list goes on. Fortunately, I reside in NYC, where you can try gastronomy from all over the world at every corner of the city. As for traveling, one of my favorite places I have traveled to was Kyoto, Japan. It was such a surreal experience I think about everyday. This is my LinkenIn, if anyone wants to follow me!"
  },
  {
    "objectID": "index.html#nice-to-meet-everybody",
    "href": "index.html#nice-to-meet-everybody",
    "title": "My First Website",
    "section": "Nice to meet everybody!",
    "text": "Nice to meet everybody!"
  },
  {
    "objectID": "Untitled.html#exploratory-analysis-data",
    "href": "Untitled.html#exploratory-analysis-data",
    "title": "Mini-Project #01: Gourmet Cheeseburgers Across the Globe",
    "section": "Exploratory Analysis Data",
    "text": "Exploratory Analysis Data\n1) How many different countries does Netflix operate in? (You can use the viewing history as a proxy for countries in which Netflix operates.)\n\n\nCode\nCountries_total &lt;- COUNTRY_TOP_10 |&gt; \n  distinct(country_name) |&gt; \n  count()\n\n\nNetflix operates in 94 countries.\n2) Which non-English-language film has spent the most cumulative weeks in the global top 10? How many weeks did it spend?\n\n\nCode\nne_mcmtv_wks &lt;- GLOBAL_TOP_10 |&gt;\n  filter(category == \"Films (Non-English)\") |&gt;\n  slice_max(cumulative_weeks_in_top_10, n=1)\n\nnon_english_title &lt;- ne_mcmtv_wks$show_title\nnon_english_weeks &lt;- ne_mcmtv_wks$cumulative_weeks_in_top_10\n\n\nThe non-English movie that has spent the most cumulative weeks in the global Top 10 is All Quiet on the Western Front for a total of 23 weeks.\n3) What is the longest film (English or non-English) to have ever appeared in the Netflix global Top 10? How long is it in minutes?\n\n\nCode\nlongest_film &lt;- GLOBAL_TOP_10 |&gt; \n  filter(str_detect(category, \"Films\")) |&gt;\n  filter(!is.na(runtime)) |&gt;\n  mutate(runtime_minutes = round(60 * runtime)) |&gt;\n  arrange(desc(runtime_minutes)) |&gt;\n  slice(1)\n\nlongest_film_name &lt;- longest_film$show_title\nlongest_film_mins &lt;- longest_film$runtime_minutes\n\n\nThe longest film to ever appear the longest in global Top 10 is Pushpa 2: The Rule (Reloaded Version) for 224 minutes.\n4) For each of the four categories, what program has the most total hours of global viewership?\n\n\nShow Code\nmost_hours_prgms &lt;- GLOBAL_TOP_10 |&gt;\n  group_by(category, show_title) |&gt; \n  summarise(total_hours = sum(weekly_hours_viewed)) |&gt;\n  group_by(category) |&gt; \n  slice_max(total_hours, n=1) |&gt;\n  ungroup() |&gt;\n  arrange(desc(total_hours))\n\n\n\n\nCode\nmost_hours_prgms |&gt;\n  mutate(total_hours = scales::comma(total_hours)) |&gt;\n  format_titles() |&gt;\n  datatable(\n    most_hours_prgms, \n    options = list(searching = FALSE, info = FALSE),\n    caption = \"Top Programs' Total Hours by Category\")\n\n\n\n\n\n\n5) Which TV show had the longest run in a country’s Top 10? How long was this run and in what country did it occur?\n\n\nCode\nlongest_cn_run &lt;- COUNTRY_TOP_10 |&gt;\n  filter(str_detect(category, \"TV\")) |&gt;\n  group_by(country_name, show_title) |&gt;\n  summarise(mx_cmwks = max(cumulative_weeks_in_top_10, na.rm = TRUE), .groups = \"drop\") |&gt;\n  arrange(desc(mx_cmwks)) |&gt;\n  slice(1)\n\nlongest_cn_run_title &lt;- longest_cn_run$show_title\nlongest_cn_run_weeks &lt;- longest_cn_run$mx_cmwks\nlongest_cn_run_name &lt;- longest_cn_run$country_name\n\n\nMoney Heist had the longest run in a Pakistan’s Top 10 for a total of 127 weeks.\n6) Netflix provides over 200 weeks of service history for all but one country in our data set. Which country is this and when did Netflix cease operations in that country?\n\n\nCode\nna_200 &lt;- COUNTRY_TOP_10 |&gt;\n  group_by(country_name) |&gt;\n  summarise(total_least_weeks = n_distinct(week), last_wk = max(week, na.rm = TRUE), .groups = \"drop\") |&gt;\n  filter(total_least_weeks &lt; 200) |&gt;\n  arrange(total_least_weeks)\n\nleast_weeks_country &lt;- na_200$country_name\nleast_weeks_date &lt;- na_200$last_wk\n\n\nOn 2022-02-27, Netflix ceased their operations in Russia.\n7) What is the total viewership of the TV show Squid Game? Note that there are three seasons total and we are looking for the total number of hours watched across all seasons.\n\n\nCode\ntotal_viewership_sg &lt;- GLOBAL_TOP_10 |&gt;\n  filter(str_detect(show_title, \"Squid Game\")) |&gt;\n  summarise(total_hours_viewed = sum(weekly_hours_viewed, na.rm = TRUE)) |&gt;\n  pull(total_hours_viewed)\ntotal_viewership_squid_game &lt;- scales::comma(total_viewership_sg)\n\n\nThe total viewership of the TV show Squid Game is 5,310,000,000 hours, taking into account all three seasons.\n8) The movie Red Notice has a runtime of 1 hour and 58 minutes. Approximately how many views did it receive in 2021? Note that Netflix does not provide the weekly_views values that far back in the past, but you can compute it yourself using the total view time and the runtime.\n\n\nCode\nlibrary(lubridate)\nRed_Notice_21 &lt;- GLOBAL_TOP_10 |&gt;\n  filter(show_title == \"Red Notice\", year(week) == 2021) |&gt;\n  summarise(total_hours = sum(weekly_hours_viewed, na.rm = TRUE)) |&gt;\n  mutate(approx_views = total_hours / (1 + 58/60))\n\n\nThe total approximate views Red Notice received in 2021 is 201,732,203 views.\n9) How many Films reached Number 1 in the US but did not originally debut there? That is, find films that first appeared on the Top 10 chart at, e.g., Number 4 but then became more popular and eventually hit Number 1? What is the most recent film to pull this off?\n\n\nCode\ntop_US_films &lt;- COUNTRY_TOP_10 |&gt;\n  filter(country_name == \"United States\", str_detect(category, \"Films\")) |&gt;\n  arrange(week) |&gt;\n  group_by(show_title) |&gt;\n  summarise(\n    debut_week = min(week, na.rm = TRUE),\n    debut_rank = first(weekly_rank),\n    top_rank = min(weekly_rank, na.rm = TRUE),\n    latest_week = max(week)\n    ) |&gt;\n  filter(debut_rank &gt; 1, top_rank == 1)\n\nlastest_film &lt;- top_US_films |&gt; \n  arrange(desc(latest_week)) |&gt;\n  slice(1) \n\ntop_US_films |&gt;\n  format_titles() |&gt;\n  datatable(\n    options = list(searching= FALSE, info= FALSE), \n    caption = \"Films That Climed to Number 1\"\n  )\n\n\n\n\n\n\nCode\ntotal_top_films &lt;- nrow(top_US_films)\nrecent_top_1 &lt;- lastest_film$show_title\n\n\nThere is a total of 45 films that did not originally debut at number 1, but eventually ranked at the top. The most recent film to achieve this was KPop Demon Hunters.\n10) Which TV show/season hit the top 10 in the most countries in its debut week? In how many countries did it chart?\n\n\nCode\ntop_show_debut &lt;- COUNTRY_TOP_10 |&gt;\n  filter(category == \"TV\") |&gt;\n  group_by(show_title) |&gt;\n  filter(week == min(week)) |&gt; \n  summarise(countries_amount = n_distinct(country_name), .groups = \"drop\") |&gt;\n  arrange(desc(countries_amount)) |&gt; \n  slice(1)\n\nshow_name &lt;- top_show_debut$show_title\ncountries_charted &lt;- top_show_debut$countries_amount\n\n\nEmily in Paris hit the Top 10 in most countries in its debut week, with a total of 94 countries charted."
  },
  {
    "objectID": "Untitled.html#the-rise-of-the-upside-down",
    "href": "Untitled.html#the-rise-of-the-upside-down",
    "title": "Mini-Project #01: Gourmet Cheeseburgers Across the Globe",
    "section": "The Rise of the Upside Down",
    "text": "The Rise of the Upside Down\n\nPress Release 1: Upcoming Season of Stranger Things\n\n\n\nStranger Things Season 5\n\n\n\n\nCode\nlibrary(lubridate)\nst_views &lt;- GLOBAL_TOP_10 |&gt;\n  filter(show_title == \"Stranger Things\") |&gt;\n  summarise(total_hours = sum(weekly_hours_viewed, na.rm = TRUE))\n\nst_data &lt;- GLOBAL_TOP_10 %&gt;%\n  filter(str_detect(show_title, regex(\"Stranger Things\", ignore_case = TRUE))) %&gt;%\n  summarise(\n    total_weeks = max(cumulative_weeks_in_top_10, na.rm = TRUE)\n  )\n\ncompare_shows &lt;- GLOBAL_TOP_10 |&gt;\n  select(show_title, weekly_hours_viewed) |&gt; \n  filter(show_title == \"Stranger Things\" | \n           show_title == \"My Life With the Walter Boys\" | \n           show_title ==\"Love Is Blind: UK\" | \n           show_title == \"You\") |&gt; \n  group_by(show_title) |&gt; \n  summarise(total_weekly_hours = sum(weekly_hours_viewed))\n\nwalter_boys &lt;- compare_shows$show_title[2]\nyou_show &lt;- compare_shows$show_title[4]\nwalter_views &lt;- compare_shows$total_weekly_hours[2]\nyou_views &lt;- compare_shows$total_weekly_hours[4]\n\n\nAt last, Stranger Things is coming to an end, releasing its final season on November 26, 2025. Not only has it been a huge hit across the United States, but across other nations as well, garnering a total viewership of 2,967,980,000 hours. Compared to other popular shows such as You and My Life With the Walter Boys, they fell behind with 1,542,990,000 and 629,800,000 hours, respectively. These are astounding numbers, as we saw it was one of the top viewed shows and also ranked highly for several weeks. It has maintained itself 19 weeks in the Top 10.\nAs a Stranger Things watcher myself, I cannot wait to see if Hawkins will be consumed by the underworld, or if it will finally be closed for good."
  },
  {
    "objectID": "Untitled.html#breaking-into-india-netflixs-ambitions-in-the-indian-market",
    "href": "Untitled.html#breaking-into-india-netflixs-ambitions-in-the-indian-market",
    "title": "Mini-Project #01: Gourmet Cheeseburgers Across the Globe",
    "section": "Breaking Into India: Netflix’s Ambitions in the Indian Market",
    "text": "Breaking Into India: Netflix’s Ambitions in the Indian Market\n\nPress Release 2: Commercial Success in India\n\n\nCode\ntop_hindi_films &lt;- GLOBAL_TOP_10 |&gt; select(show_title, weekly_hours_viewed) |&gt; \n  filter(str_detect(show_title, \"Hindi\")) |&gt; \n  group_by(show_title) |&gt; \n  summarise(total_wkly_hrs = sum(weekly_hours_viewed)) |&gt;\n  slice_max(total_wkly_hrs, n = 5)\n  \nhindi_india &lt;- COUNTRY_TOP_10 %&gt;%\n  filter(country_name == \"India\") %&gt;%\n  filter(str_detect(show_title, regex(\"Hindi\", ignore_case = TRUE)) |\n         str_detect(season_title, regex(\"Hindi\", ignore_case = TRUE))) %&gt;%\n  group_by(show_title) %&gt;%\n  summarise(\n    weeks_top10 = max(cumulative_weeks_in_top_10, na.rm = TRUE),\n    peak_rank   = min(weekly_rank, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %&gt;%\n  arrange(peak_rank, desc(weeks_top10))\n\nrrr &lt;- top_hindi_films$show_title[1]\ntop_hindi_2 &lt;- top_hindi_films$show_title[2]\nrrr_hours &lt;-top_hindi_films$total_wkly_hrs[1]\ntop2_hours &lt;- top_hindi_films$total_wkly_hrs[2]\nrrr_rank_weeks &lt;- hindi_india$weeks_top10[1]\n\n\nBeing the most populated country in the world, India is an enticing market many companies are ready to enter and invest in, Netlfix being one of them. Currently RRR (Hindi) and Kalki 2898 AD (Hindi) sit at the top with 79,780,000 and 23,100,000 hours, respectively. Not to mention RRR (Hindi) has spent 25 weeks ranked 1. A total of 11 films have ranked that high, attracting Netflix even more."
  },
  {
    "objectID": "Untitled.html#global-hits-non-english-tv-shows-taking-over-worldwide",
    "href": "Untitled.html#global-hits-non-english-tv-shows-taking-over-worldwide",
    "title": "Mini-Project #01: Gourmet Cheeseburgers Across the Globe",
    "section": "Global Hits: Non-English TV Shows Taking Over Worldwide",
    "text": "Global Hits: Non-English TV Shows Taking Over Worldwide\n\nPress Release 3: Open Topic\n\n\nCode\nne__shows &lt;- GLOBAL_TOP_10 |&gt;\n  filter(category == \"TV (Non-English)\") |&gt;\n  group_by(show_title) |&gt;\n  summarise(\n    total_weeks = max(cumulative_weeks_in_top_10, na.rm = TRUE),\n    total_hours = sum(weekly_hours_viewed, na.rm = TRUE),\n    .groups = \"drop\"\n  ) |&gt;\n  arrange(desc(total_weeks))\n\nne__shows |&gt;\n  mutate(total_hours = scales::comma(total_hours)) |&gt;\n  head(10) |&gt;\n  format_titles() |&gt;\n  datatable(\n    options = list(searching= FALSE, info= FALSE), \n    caption = \"Top Non-English Shows\"\n  )\n\n\n\n\n\n\nCode\nbetty_show &lt;- ne__shows$show_title[2]\nbetty_wks &lt;- ne__shows$total_weeks[2]\nbetty_hours&lt;- ne__shows$total_hours[2]\n\n\nOver the last decade, we have seen an influx of foreign consumption worldwide. The table above demonstrates the TV shows’ amount of weeks spent in the Top 10. Aside from genre, there is also a variety of language in just these five shows, like Korean, Spanish, and Portuguese. Squid Game did extremly well, but Yo soy Betty, la fea did exceptional as well, with 30 weeks in the Top 10 and 297,560,000 hours in viewership. These Korean dramas and Spanish thrillers continue breaking barriers and dominating global charts."
  },
  {
    "objectID": "mp02.html",
    "href": "mp02.html",
    "title": "Making Backyards Affordable for All",
    "section": "",
    "text": "Introduction\nThroughout this mini-project, we will analyze how several factors such as, income, wages, and population, are interconnected with housing affordability. With the help of visualizations, we will be able to see different trends which in turn will help make a decision on who will our next sponsor for YIMBY.\n\n\nData Acquisition\n\nTask 1: Data Import\n\n\nShow Data Code\nif(!dir.exists(file.path(\"data\", \"mp02\"))){\n    dir.create(file.path(\"data\", \"mp02\"), showWarnings=FALSE, recursive=TRUE)\n}\n\nlibrary &lt;- function(pkg){\n    ## Mask base::library() to automatically install packages if needed\n    ## Masking is important here so downlit picks up packages and links\n    ## to documentation\n    pkg &lt;- as.character(substitute(pkg))\n    options(repos = c(CRAN = \"https://cloud.r-project.org\"))\n    if(!require(pkg, character.only=TRUE, quietly=TRUE)) install.packages(pkg)\n    stopifnot(require(pkg, character.only=TRUE, quietly=TRUE))\n}\n\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nShow Data Code\nlibrary(glue)\nlibrary(readxl)\nlibrary(tidycensus)\n\nget_acs_all_years &lt;- function(variable, geography=\"cbsa\",\n                              start_year=2009, end_year=2023){\n    fname &lt;- glue(\"{variable}_{geography}_{start_year}_{end_year}.csv\")\n    fname &lt;- file.path(\"data\", \"mp02\", fname)\n    \n    if(!file.exists(fname)){\n        YEARS &lt;- seq(start_year, end_year)\n        YEARS &lt;- YEARS[YEARS != 2020] # Drop 2020 - No survey (covid)\n        \n        ALL_DATA &lt;- map(YEARS, function(yy){\n            tidycensus::get_acs(geography, variable, year=yy, survey=\"acs1\") |&gt;\n                mutate(year=yy) |&gt;\n                select(-moe, -variable) |&gt;\n                rename(!!variable := estimate)\n        }) |&gt; bind_rows()\n        \n        write_csv(ALL_DATA, fname)\n    }\n    \n    read_csv(fname, show_col_types=FALSE)\n}\n\n# Household income (12 month)\nINCOME &lt;- get_acs_all_years(\"B19013_001\") |&gt;\n    rename(household_income = B19013_001)\n\n# Monthly rent\nRENT &lt;- get_acs_all_years(\"B25064_001\") |&gt;\n    rename(monthly_rent = B25064_001)\n\n# Total population\nPOPULATION &lt;- get_acs_all_years(\"B01003_001\") |&gt;\n    rename(population = B01003_001)\n\n# Total number of households\nHOUSEHOLDS &lt;- get_acs_all_years(\"B11001_001\") |&gt;\n    rename(households = B11001_001)\n\n\nAdditionally, the number of new housing units built each year is necessary for this report.\n\n\nCode\nget_building_permits &lt;- function(start_year = 2009, end_year = 2023){\n    fname &lt;- glue(\"housing_units_{start_year}_{end_year}.csv\")\n    fname &lt;- file.path(\"data\", \"mp02\", fname)\n    \n    if(!file.exists(fname)){\n        HISTORICAL_YEARS &lt;- seq(start_year, 2018)\n        \n        HISTORICAL_DATA &lt;- map(HISTORICAL_YEARS, function(yy){\n            historical_url &lt;- glue(\"https://www.census.gov/construction/bps/txt/tb3u{yy}.txt\")\n                \n            LINES &lt;- readLines(historical_url)[-c(1:11)]\n\n            CBSA_LINES &lt;- str_detect(LINES, \"^[[:digit:]]\")\n            CBSA &lt;- as.integer(str_sub(LINES[CBSA_LINES], 5, 10))\n\n            PERMIT_LINES &lt;- str_detect(str_sub(LINES, 48, 53), \"[[:digit:]]\")\n            PERMITS &lt;- as.integer(str_sub(LINES[PERMIT_LINES], 48, 53))\n            \n            data_frame(CBSA = CBSA,\n                       new_housing_units_permitted = PERMITS, \n                       year = yy)\n        }) |&gt; bind_rows()\n        \n        CURRENT_YEARS &lt;- seq(2019, end_year)\n        \n        CURRENT_DATA &lt;- map(CURRENT_YEARS, function(yy){\n            current_url &lt;- glue(\"https://www.census.gov/construction/bps/xls/msaannual_{yy}99.xls\")\n            \n            temp &lt;- tempfile()\n            \n            download.file(current_url, destfile = temp, mode=\"wb\")\n            \n            fallback &lt;- function(.f1, .f2){\n                function(...){\n                    tryCatch(.f1(...), \n                             error=function(e) .f2(...))\n                }\n            }\n            \n            reader &lt;- fallback(read_xlsx, read_xls)\n            \n            reader(temp, skip=5) |&gt;\n                na.omit() |&gt;\n                select(CBSA, Total) |&gt;\n                mutate(year = yy) |&gt;\n                rename(new_housing_units_permitted = Total)\n        }) |&gt; bind_rows()\n        \n        ALL_DATA &lt;- rbind(HISTORICAL_DATA, CURRENT_DATA)\n        \n        write_csv(ALL_DATA, fname)\n        \n    }\n    \n    read_csv(fname, show_col_types=FALSE)\n}\n\nPERMITS &lt;- get_building_permits()\n\n\nUsing the following code will allow us refer to useful income estimates from the Bureau of Labor Statistics(BLS). This code fill download such data from the North American Industry Classification System (NAICS). Ultimately, the goal is to know the best Core-Based Statistical Areas (CBSA) to live in.\n\n\nCode\nlibrary(httr2)\nlibrary(rvest)\nget_bls_industry_codes &lt;- function(){\n    fname &lt;- file.path(\"data\", \"mp02\", \"bls_industry_codes.csv\")\n    library(dplyr)\n    library(tidyr)\n    library(readr)\n    \n    if(!file.exists(fname)){\n        \n        resp &lt;- request(\"https://www.bls.gov\") |&gt; \n            req_url_path(\"cew\", \"classifications\", \"industry\", \"industry-titles.htm\") |&gt;\n            req_headers(`User-Agent` = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:143.0) Gecko/20100101 Firefox/143.0\") |&gt; \n            req_error(is_error = \\(resp) FALSE) |&gt;\n            req_perform()\n        \n        resp_check_status(resp)\n        \n        naics_table &lt;- resp_body_html(resp) |&gt;\n            html_element(\"#naics_titles\") |&gt; \n            html_table() |&gt;\n            mutate(title = str_trim(str_remove(str_remove(`Industry Title`, Code), \"NAICS\"))) |&gt;\n            select(-`Industry Title`) |&gt;\n            mutate(depth = if_else(nchar(Code) &lt;= 5, nchar(Code) - 1, NA)) |&gt;\n            filter(!is.na(depth))\n        \n        # These were looked up manually on bls.gov after finding \n        # they were presented as ranges. Since there are only three\n        # it was easier to manually handle than to special-case everything else\n        naics_missing &lt;- tibble::tribble(\n            ~Code, ~title, ~depth, \n            \"31\", \"Manufacturing\", 1,\n            \"32\", \"Manufacturing\", 1,\n            \"33\", \"Manufacturing\", 1,\n            \"44\", \"Retail\", 1, \n            \"45\", \"Retail\", 1,\n            \"48\", \"Transportation and Warehousing\", 1, \n            \"49\", \"Transportation and Warehousing\", 1\n        )\n        \n        naics_table &lt;- bind_rows(naics_table, naics_missing)\n        \n        naics_table &lt;- naics_table |&gt; \n            filter(depth == 4) |&gt; \n            rename(level4_title=title) |&gt; \n            mutate(level1_code = str_sub(Code, end=2), \n                   level2_code = str_sub(Code, end=3), \n                   level3_code = str_sub(Code, end=4)) |&gt;\n            left_join(naics_table, join_by(level1_code == Code)) |&gt;\n            rename(level1_title=title) |&gt;\n            left_join(naics_table, join_by(level2_code == Code)) |&gt;\n            rename(level2_title=title) |&gt;\n            left_join(naics_table, join_by(level3_code == Code)) |&gt;\n            rename(level3_title=title) |&gt;\n            select(-starts_with(\"depth\")) |&gt;\n            rename(level4_code = Code) |&gt;\n            select(level1_title, level2_title, level3_title, level4_title, \n                   level1_code,  level2_code,  level3_code,  level4_code) |&gt;\n            drop_na() |&gt;\n            mutate(across(contains(\"code\"), as.integer))\n        \n        write_csv(naics_table, fname)\n    }\n    \n    read_csv(fname, show_col_types=FALSE)\n}\n\nINDUSTRY_CODES &lt;- get_bls_industry_codes()\n\n\nFinally, we need to obtain the BLS Quarterly Census of Employment and Wages.\n\n\nCode\nlibrary(httr2)\nlibrary(rvest)\nget_bls_qcew_annual_averages &lt;- function(start_year=2009, end_year=2023){\n    fname &lt;- glue(\"bls_qcew_{start_year}_{end_year}.csv.gz\")\n    fname &lt;- file.path(\"data\", \"mp02\", fname)\n    \n    YEARS &lt;- seq(start_year, end_year)\n    YEARS &lt;- YEARS[YEARS != 2020] # Drop Covid year to match ACS\n    \n    if(!file.exists(fname)){\n        ALL_DATA &lt;- map(YEARS, .progress=TRUE, possibly(function(yy){\n            fname_inner &lt;- file.path(\"data\", \"mp02\", glue(\"{yy}_qcew_annual_singlefile.zip\"))\n            \n            if(!file.exists(fname_inner)){\n                request(\"https://www.bls.gov\") |&gt; \n                    req_url_path(\"cew\", \"data\", \"files\", yy, \"csv\",\n                                 glue(\"{yy}_annual_singlefile.zip\")) |&gt;\n                    req_headers(`User-Agent` = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:143.0) Gecko/20100101 Firefox/143.0\") |&gt; \n                    req_retry(max_tries=5) |&gt;\n                    req_perform(fname_inner)\n            }\n            \n            if(file.info(fname_inner)$size &lt; 755e5){\n                warning(sQuote(fname_inner), \"appears corrupted. Please delete and retry this step.\")\n            }\n            \n            read_csv(fname_inner, \n                     show_col_types=FALSE) |&gt; \n                mutate(YEAR = yy) |&gt;\n                select(area_fips, \n                       industry_code, \n                       annual_avg_emplvl, \n                       total_annual_wages, \n                       YEAR) |&gt;\n                filter(nchar(industry_code) &lt;= 5, \n                       str_starts(area_fips, \"C\")) |&gt;\n                filter(str_detect(industry_code, \"-\", negate=TRUE)) |&gt;\n                mutate(FIPS = area_fips, \n                       INDUSTRY = as.integer(industry_code), \n                       EMPLOYMENT = as.integer(annual_avg_emplvl), \n                       TOTAL_WAGES = total_annual_wages) |&gt;\n                select(-area_fips, \n                       -industry_code, \n                       -annual_avg_emplvl, \n                       -total_annual_wages) |&gt;\n                # 10 is a special value: \"all industries\" , so omit\n                filter(INDUSTRY != 10) |&gt; \n                mutate(AVG_WAGE = TOTAL_WAGES / EMPLOYMENT)\n        })) |&gt; bind_rows()\n        \n        write_csv(ALL_DATA, fname)\n    }\n    \n    ALL_DATA &lt;- read_csv(fname, show_col_types=FALSE)\n    \n    ALL_DATA_YEARS &lt;- unique(ALL_DATA$YEAR)\n    \n    YEARS_DIFF &lt;- setdiff(YEARS, ALL_DATA_YEARS)\n    \n    if(length(YEARS_DIFF) &gt; 0){\n        stop(\"Download failed for the following years: \", YEARS_DIFF, \n             \". Please delete intermediate files and try again.\")\n    }\n    \n    ALL_DATA\n}\n\nWAGES &lt;- get_bls_qcew_annual_averages()\n\n\n\n\n\n\nData Integration and Initial Exploration\n\n\n\nRelationship Diagram\n\n\n\nTask 2: Multi-Table Questions\n1. Which CBSA (by name) permitted the largest number of new housing units in the decade from 2010 to 2019 (inclusive)?\n\n\nCode\nlibrary(dplyr)\n\nnew_housing_units1019 &lt;- PERMITS |&gt;\n  filter(year &gt;= 2010, year &lt;= 2019) |&gt;\n  group_by(CBSA) |&gt;\n  summarise(total_permits_given = sum(new_housing_units_permitted, na.rm = TRUE)) |&gt;\n  arrange(desc(total_permits_given)) |&gt;\n  left_join(POPULATION |&gt; select(GEOID, NAME) |&gt; distinct(), \n            by = c(\"CBSA\" = \"GEOID\")) |&gt;\n  ungroup()\n\ntop_cities &lt;- new_housing_units1019 |&gt; arrange(desc(total_permits_given)) |&gt; slice_max(total_permits_given, n=10)\n  \nlibrary(ggplot2)\nggplot(top_cities, aes(x = reorder(NAME, total_permits_given), y = total_permits_given)) +\n  geom_col(fill = \"steelblue\", width = 0.5) +\n  coord_flip() +\n  scale_y_continuous(labels = scales::comma) +\n  labs(\n    title = \"Total New Housing Units Permited (2010-2019)\",\n    x = \"Metropolitan Area (CBSA)\",\n    y = \"Total Permits Issued\"\n  ) +\n  theme_minimal(base_size = 9)\n\n\n\n\n\n\n\n\n\nCode\ntop_city &lt;- new_housing_units1019$total_permits_given[1]\ntop_city_name &lt;- new_housing_units1019$NAME[1]\n\n\nThe largest number of new housing units in the decade from 2010 to 2019 was in Houston-Sugar Land-Baytown, TX Metro Area with a total of 482,075 permits issued.\n2. In what year did Albuquerque, NM (CBSA Number 10740) permit the most new housing units?\n\n\nCode\nlibrary(DT)\n\nalbuquerque_new_permits &lt;- PERMITS |&gt; \n  filter(CBSA == 10740) |&gt;\n  group_by(year) |&gt;\n  summarise(ABQ_total_permits = sum(new_housing_units_permitted, na.rm = TRUE)) |&gt;\n  arrange(desc(ABQ_total_permits)) |&gt;\n  ungroup()\n\nggplot(albuquerque_new_permits, aes(x = year, y = ABQ_total_permits)) +\n  geom_col(fill = \"steelblue\") +\n  labs(\n    title = \"Albuquerque New Housing Units Permitted by Year\",\n    x = \"Year\",\n    y = \"Total Permits\"\n  ) +\n  theme_minimal(base_size = 10)\n\n\n\n\n\n\n\n\n\nCode\nabq_newest_top_year &lt;- albuquerque_new_permits$year[1]\nabq_new_total_permits &lt;- albuquerque_new_permits$ABQ_total_permits[1]\n\n\nIn Albuquerque, NM, 2021 was the year in which new housing units were permitted the most, with a total of 4,021 permits issued.\n3. Which state (not CBSA) had the highest average individual income in 2015?\n\n\nCode\nlibrary(dplyr)\nhighest_avg_ind_income &lt;- INCOME |&gt;\n  filter(year == 2015) |&gt;\n  left_join(HOUSEHOLDS, \n            by = c(\"GEOID\", \"NAME\", \"year\")) |&gt;\n  left_join(POPULATION, \n            by = c(\"GEOID\", \"NAME\", \"year\")) |&gt;\n  mutate(state = str_extract(NAME, \", (.{2})\", group=1),\n         total_income_CBSA = household_income*households) |&gt;\n  group_by(state) |&gt; \n  summarise(\n    state_total_income = sum(total_income_CBSA, na.rm = TRUE),\n    total_population = sum(population, na.rm = TRUE), \n    average_individual_income = state_total_income/total_population\n  ) |&gt;\n  arrange(desc(average_individual_income)) \n\nlibrary(scales)\nlibrary(DT)\n\ntop_states &lt;- highest_avg_ind_income |&gt; \n  arrange(desc(average_individual_income)) |&gt; \n  slice_max(average_individual_income, n = 5) |&gt; \n  mutate(\n    state_total_income = dollar(state_total_income),\n    total_population = comma(total_population),\n    average_individual_income = dollar(average_individual_income)\n  )\n\ncolnames(top_states) &lt;- str_replace_all(colnames(top_states), \"_\", \" \") |&gt;\n  str_to_title()\n\ndatatable(\n  top_states,\n  options = list(searching = FALSE, info = FALSE),\n  caption = \"Top 5 Highest Average Individual Income by State (2015)\"\n)\n\n\n\n\n\n\nThe state with the highest average individual income is DC, with an average of $33,232.88.\n4. Data scientists and business analysts are recorded under NAICS code 5182. What is the last year in which the NYC CBSA had the most data scientists in the country? In recent, the San Francisco CBSA has had the most data scientists.\n\n\nCode\nlibrary(dplyr)\nfiltered_ind &lt;- WAGES |&gt;\n  filter(INDUSTRY == 5182) |&gt; \n  group_by(YEAR, FIPS) |&gt;\n  summarise(total_employment = sum(EMPLOYMENT, na.rm = TRUE)) |&gt;\n  filter(total_employment == max(total_employment)) |&gt;\n  mutate(CBSA = paste0(FIPS, \"0\"))\n\nfiltered_population &lt;- POPULATION |&gt;\n  mutate(CBSA = paste0(\"C\", GEOID))\n\nlibrary(DT)\nlibrary(scales)\nData_Scientist &lt;- inner_join(\n  filtered_ind,\n  filtered_population,\n  join_by(CBSA == CBSA, YEAR == year))|&gt; \n  group_by(YEAR, NAME) |&gt; select(-population, -FIPS, -GEOID) |&gt;\n  filter(str_detect(NAME, \"New York\")) |&gt;\n  arrange(desc(total_employment)) |&gt; \n  mutate(total_employment = comma(total_employment))\n\ntop_ds_year &lt;- Data_Scientist$Year[1]\ntop_ds_te &lt;- Data_Scientist$`Total Employment`[1]\n\ncolnames(Data_Scientist) &lt;- str_replace_all(colnames(Data_Scientist), \"_\", \" \") |&gt;\n  str_to_title()\n\ndatatable(\n  Data_Scientist,\n  options = list(searching = FALSE, info = FALSE),\n  caption = \"NYC CBSA Data Scientists\"\n)\n\n\n\n\n\n\nThe last year in which the NYC CBSA had the most data scientists in the country was in 2015, with a total of 18,922 employees.\n5. What fraction of total wages in the NYC CBSA was earned by people employed in the finance and insurance industries (NAICS code 52)? In what year did this fraction peak?\n\n\nCode\nwages_nyc &lt;- WAGES |&gt;\n  filter(FIPS == \"C3562\") |&gt; \n  group_by(YEAR) |&gt;\n  summarise(total_wages_nyc = sum(TOTAL_WAGES, na.rm = TRUE),\n            finance_and_insurance_wages = sum(ifelse(INDUSTRY == 52, TOTAL_WAGES,0)),\n            finance_fraction= finance_and_insurance_wages/total_wages_nyc)|&gt;\n  mutate(\n    total_wages_nyc = dollar(total_wages_nyc),\n    finance_and_insurance_wages = dollar(finance_and_insurance_wages),\n    finance_fraction = percent(finance_fraction)) |&gt;\n  arrange(desc(finance_fraction))\n\ncolnames(wages_nyc) &lt;- str_replace_all(colnames(wages_nyc), \"_\", \" \") |&gt;\n  str_to_title()\n\ndatatable(\n  wages_nyc,\n  options = list(searching = FALSE, info = FALSE),\n  caption = \"Finance and Insurance Wages and Fractions\"\n)\n\n\n\n\n\n\nCode\npeak_year &lt;- wages_nyc$Year[1]\npeak_fraction &lt;- wages_nyc$`Finance Fraction`[1]\n\n\nApproximately 4.6038% of total wages was earned by people employed in the finance and insurance industries, and they peaked in 2014.\n\n\nTask 3: Initial Visualizations\nVisualization 1\n\n\n\n\n\n\n\n\n\nVisualization 2\n\n\nCode\nhealth_employment &lt;- WAGES |&gt;\n  mutate(healthcare = INDUSTRY == 62, \n         cbsa = paste0(FIPS, \"0\")) |&gt;\n  group_by(cbsa,YEAR) |&gt;\n  summarise(tt_hh_ss_employment = sum(EMPLOYMENT[healthcare], na.rm = TRUE),\n            total_employment = sum(EMPLOYMENT, na.rm = TRUE), .groups = \"drop\") |&gt;\n  filter(total_employment &gt; 0, tt_hh_ss_employment &gt; 0)\n\nlibrary(ggplot2)\nlibrary(scales)\nggplot(health_employment, aes(x = total_employment/1000, y = tt_hh_ss_employment/1000)) +\n  geom_point(aes(color = cbsa),alpha = 0.2, show.legend = FALSE) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red4\", linewidth = 0.7) +\n  facet_wrap(~YEAR, ncol = 3)+\n  labs(\n    title = \"Healthcare Employment vs Total Employment\",\n    subtitle = \"Across Different CBSAs\",\n    x = \"Total Employment\",\n    y = \"Total H and S Employment\",\n    caption = \"idl yet\"\n  ) +\n  scale_x_continuous(labels = comma_format()) +\n  scale_y_continuous(labels = comma_format()) +\n  theme_minimal(base_size = 10) +\n  theme_bw() + \n  theme(\n    plot.title = element_text(face = \"bold\"))\n\n\n\n\n\n\n\n\n\nVisualization 3\n\n\nCode\nHousehold_trend &lt;- POPULATION |&gt;\n  mutate(CBSA = paste0(GEOID, \"0\")) |&gt;\n  inner_join(HOUSEHOLDS, by = c(\"GEOID\", \"NAME\", \"year\")) |&gt;\n  mutate(avg_hh_size = population/households) |&gt;\n  arrange(desc(avg_hh_size))\n\nlibrary(gghighlight)\n  \nggplot(Household_trend, aes(\n  x = year, y = avg_hh_size,\n  group = CBSA, color = case_when(CBSA == 356200 ~ \"NY-NJ Metro Area\",\n                                  CBSA == 310800 ~ \"CA Metro Area\")\n)) +\n  geom_line() +\n  scale_color_manual(values = c(\"NY-NJ Metro Area\" = \"steelblue\", \"CA Metro Area\" = \"firebrick\"),\n                     name = \"Highlight\") +\n  gghighlight(CBSA %in% c(356200,310800), \n              use_direct_label = FALSE, \n              unhighlighted_params = list(alpha = 0.3)) +\n  scale_x_continuous(\n    breaks = seq(2009, 2023, 2)\n  ) +\n  labs(\n    title = \"Evolution of Average Household Size Over Time (2009-2023)\",\n    subtitle = \"Across Different CBSAs\",\n    x = \"Year\",\n    y = \"Average Household Size\",\n    caption = \"will come up iwth one\"\n  ) +\n  theme_bw() +\n  theme(\n    plot.title = element_text(face = \"bold\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding Indices of Housing Affordability and housing Stock Growth\n\nTask 4: Rent Burden\n\n\nCode\nlibrary(dplyr)\nlibrary(DT)\nrent_burden &lt;- INCOME |&gt;\n  inner_join(RENT, by = c(\"GEOID\", \"NAME\", \"year\")) |&gt;\n  inner_join(POPULATION, by = c(\"GEOID\", \"NAME\", \"year\"))|&gt;\n  filter(!is.na(household_income), !is.na(monthly_rent)) |&gt;\n  mutate(\n    rent_income_percentage = (monthly_rent * 12 / household_income)) |&gt; \n  select(-household_income, \n         -monthly_rent) |&gt;\n  mutate(rent_burden_index = scales::rescale(rent_income_percentage, to = c(0, 100))) \n\ncolnames(rent_burden) &lt;- str_replace_all(colnames(rent_burden), \"_\", \" \") |&gt;\n  str_to_title()\n\n\nhighest_burden &lt;- max(rent_burden$`Rent Income Percentage`, na.rm = TRUE)\nlowest_burden &lt;- min(rent_burden$`Rent Income Percentage`, na.rm = TRUE)\nhighest_name &lt;- rent_burden$Name[4976]\nlowest_name &lt;- rent_burden$Name[49]\n\n#Ohio Metro Area\nohio_metro_area &lt;- rent_burden |&gt; filter(Geoid == 26580)\n\ndatatable(\n  ohio_metro_area,\n  options = list(searching = FALSE, info = FALSE),\n  caption = \"Ohio Rent Burden\" ) |&gt; formatPercentage(columns = \"Rent Income Percentage\", digits = 2) |&gt;\n  formatRound(columns = \"Rent Burden Index\", digits = 2)\n\n\n\n\n\n\nthis is my second table highlighting metro area.\n\n\nCode\nhighest &lt;- rent_burden |&gt; \n  filter(`Rent Income Percentage` == max(`Rent Income Percentage`, na.rm = TRUE)) |&gt; \n  mutate(Level = \"Highest Burden\")\n\nlowest &lt;- rent_burden |&gt; \n  filter(`Rent Income Percentage` == min(`Rent Income Percentage`, na.rm = TRUE)) |&gt; \n  mutate(Level = \"Lowest Burden\")\n\nburden_summary &lt;- bind_rows(highest, lowest) |&gt; \n  select(Level, Name, Geoid, Year, `Rent Income Percentage`, `Rent Burden Index`)\n\ndatatable(\n  burden_summary,\n  caption = \"Areas with Highest and Lowest Rent Burden\",\n  options = list(pageLength = 5, searching = FALSE, info = FALSE)\n) |&gt;\n  formatPercentage(columns = \"Rent Income Percentage\", digits = 2) |&gt;\n  formatRound(columns = \"Rent Burden Index\", digits = 2)\n\n\n\n\n\n\n\n\nTask 5: Housing Growth\n\n\nCode\nlibrary(dplyr)\nlibrary(RcppRoll)\n\n# 1. Join POPULATION and PERMITS tables\nhousing_growth &lt;- POPULATION %&gt;%\n  inner_join(PERMITS, join_by(GEOID == CBSA, year == year)) %&gt;%\n  arrange(GEOID, year) %&gt;%\n  group_by(GEOID) %&gt;%\n  mutate(\n    # 5-year lagged population and population growth\n    population_5yr_ago = lag(population, 5),\n    population_growth_5yr = population - population_5yr_ago,\n    base_year = year &gt;= 2014\n  ) %&gt;%\n  ungroup()\n\n# 2. Compute national averages for standardization\nnational_stats &lt;- list(\n  permits_mean = mean(housing_growth$new_housing_units_permitted, na.rm = TRUE),\n  pop_mean = mean(housing_growth$population, na.rm = TRUE),\n  growth_mean = mean(housing_growth$population_growth_5yr[housing_growth$base_year], na.rm = TRUE)\n)\n\n# 3. Construct metrics\nhousing_growth &lt;- housing_growth %&gt;%\n  mutate(\n    # --- Instantaneous Metric ---\n    # Scale by population and national average\n    permits_per_1000_pop = (new_housing_units_permitted / population) * 1000,\n    national_permits_per_1000 = (national_stats$permits_mean / national_stats$pop_mean) * 1000,\n    instantaneous_metric = (permits_per_1000_pop / national_permits_per_1000) * 50,\n    \n    # --- Rate-Based Metric ---\n    # Ratio of permits to 5-year population growth, standardized to national average\n    permits_to_growth_ratio = new_housing_units_permitted / pmax(population_growth_5yr),\n    rate_based_metric = if_else(\n      base_year,\n      (permits_to_growth_ratio / mean(permits_to_growth_ratio, na.rm = TRUE)) * 50,\n      NA_real_\n    ),\n    \n    # --- Composite Metric ---\n    composite_index = if_else(\n      base_year,\n      (instantaneous_metric + rate_based_metric) / 2,\n      NA_real_\n    )\n  ) %&gt;%\n  group_by(GEOID) %&gt;%\n  # 5-year rolling average of composite metric\n  mutate(composite_index_rolling = roll_mean(composite_index, n = 5, align = \"right\", fill = NA)) %&gt;%\n  ungroup()\n\nlibrary(dplyr)\nlibrary(DT)\n\n# Create simplified Instantaneous Metric Table\ninstantaneous_table &lt;- housing_growth %&gt;%\n  select(\n    CBSA = GEOID,\n    Name = NAME,       # Adjust if your name column is different\n    year,\n    permits_per_1000_pop,\n    instantaneous_metric\n  ) %&gt;%\n  arrange(desc(permits_per_1000_pop))\n\ncolnames(instantaneous_table) &lt;- str_replace_all(colnames(instantaneous_table), \"_\", \" \") |&gt;\n  str_to_title()\n\n# Display interactive DT table\ndatatable(\n  instantaneous_table,\n  rownames = FALSE,\n  filter = \"top\",\n  options = list(pageLength = 15, autoWidth = TRUE, scrollX = TRUE),\n  caption = \"CBSA Instantaneous Housing Growth Metrics\"\n)   |&gt; formatRound(columns = \"Permits Per 1000 Pop\", digits = 2) |&gt; formatRound(columns = \"Instantaneous Metric\", digits = 2)\n\n\n\n\n\n\nThe Instantaneous index allows us to see the permits per 1,000 residents, which ultimately tells us how active construction is right now.\n\n\nCode\nlibrary(dplyr)\nlibrary(DT)\n\n# Create simplified Rate-Based Metric Table\nrate_based_table &lt;- housing_growth %&gt;%\n  filter(base_year) %&gt;%  # Only include years with 5-year growth\n  select(\n    CBSA = GEOID,\n    Name = NAME,   # Adjust if your column has a different name\n    year,\n    permits_to_growth_ratio,\n    rate_based_metric\n  ) %&gt;%\n  arrange(desc(permits_to_growth_ratio))\n\ncolnames(rate_based_table) &lt;- str_replace_all(colnames(rate_based_table), \"_\", \" \") |&gt;\n  str_to_title()\n\n# Display interactive DT table\ndatatable(\n  rate_based_table,\n  rownames = FALSE,\n  filter = \"top\",\n  options = list(pageLength = 15, autoWidth = TRUE, scrollX = TRUE),\n  caption = \"CBSA Rate-Based Housing Growth Metrics\"\n) |&gt; formatRound(columns = \"Permits To Growth Ratio\", digits = 2) |&gt; formatRound(columns = \"Rate Based Metric\", digits = 2)\n\n\n\n\n\n\nRate-based index indicated a population growth per 5-year which tells us how well supply tracks demand.\n\n\nTask 6: Visualization\n\n\nCode\nlibrary(dplyr)\n\n# Make sure column names match your datasets\nrent_burden_clean &lt;- rent_burden |&gt; \n  rename(GEOID = Geoid, year = Year, rent_burden_index = `Rent Burden Index`, \n         rent_income_percentage = `Rent Income Percentage`, Name = Name)\n\n# Merge rent burden and housing growth\ncombined &lt;- housing_growth %&gt;%\n  inner_join(rent_burden_clean, by = c(\"NAME\" = \"Name\", \"GEOID\" = \"GEOID\", \"year\" = \"year\")\n) %&gt;%\n  group_by(GEOID, NAME) %&gt;%\n  mutate(\n    rent_burden_change = rent_burden_index - lag(rent_burden_index, 5),\n    population_change = population - lag(population, 5)\n  ) %&gt;%\n  ungroup()\n\nlibrary(ggplot2)\n\n# Calculate early rent burden (average in first 5 years)\nearly_burden &lt;- combined %&gt;%\n  filter(year &lt;= min(year) + 4) %&gt;%\n  group_by(GEOID, NAME) %&gt;%\n  summarize(avg_rent_burden_early = mean(rent_burden_index, na.rm = TRUE)) %&gt;%\n  ungroup()\n\n# Join back to get one row per CBSA for latest year\nplot_data &lt;- combined %&gt;%\n  filter(year == max(year)) %&gt;%\n  inner_join(early_burden, by = c(\"GEOID\", \"NAME\"))\n\nggplot(plot_data, aes(\n  x = composite_index_rolling,\n  y = rent_burden_change,\n  size = population_change,\n  color = avg_rent_burden_early\n)) +\n  geom_point(alpha = 0.7) +\n  scale_color_viridis_c(option = \"plasma\", name = \"Early Rent Burden\") +\n  scale_size_continuous(name = \"Population Growth\") +\n  labs(\n    title = \"Rent Burden Change vs. Housing Growth (YIMBY Analysis)\",\n    subtitle = \"CBSAs with high early rent burden, decreasing rents, and strong housing growth are YIMBY successes\",\n    x = \"Composite Housing Growth Index (5-Year Rolling Avg)\",\n    y = \"Change in Rent Burden (↓ = Rent Relief)\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Identify YIMBY candidate CBSAs\nyimby_candidates &lt;- plot_data %&gt;%\n  filter(\n    avg_rent_burden_early &gt; mean(avg_rent_burden_early, na.rm = TRUE),\n    rent_burden_change &lt; 0,\n    composite_index_rolling &gt; mean(composite_index_rolling, na.rm = TRUE),\n    population_change &gt; 0\n  ) %&gt;%\n  arrange(rent_burden_change) %&gt;%\n  slice_head(n = 5) %&gt;%\n  pull(GEOID)\n\n# Time series for selected CBSAs\nggplot(combined %&gt;% filter(GEOID %in% yimby_candidates), aes(x = year)) +\n  geom_line(aes(y = rent_burden_index, color = \"Rent Burden\"), size = 0.6) +\n  geom_line(aes(y = composite_index_rolling/2, color = \"Housing Growth (Scaled)\"), size = 0.6) +\n  facet_wrap(~NAME, scales = \"free_y\") +\n  scale_color_manual(values = c(\"Rent Burden\" = \"red\", \"Housing Growth (Scaled)\" = \"blue\")) +\n  labs(\n    title = \"Rent Burden vs. Housing Growth Over Time\",\n    subtitle = \"YIMBY CBSAs show rising housing growth and declining rent burden\",\n    y = \"Index (Scaled)\",\n    color = \"Metric\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\nPolicy Brief\n\nTask 7\n\nTitle: Adopting More YIMBY Housing Policies\n\n\nExecutive Summary\nThrough our findings, we were able to find correlations between housing, wages, income, industries, rent, and population. Overall, we discovered the relationships between rent burden, housing growth and population growth. The goal of the policy is to refer to cities which will actively grow in the sectors of housing, those whose rent will not increase to unattainable standards, and will encourage outsiders to move in, supporting a ethical and sustainable growth.\n\n\nProposed Congressional Sponsors:\nThere are multiple YIMBY success candidates that we found across our research, as we can see in the previous graph highlighting rising in housing growth and decrease in rent burden. The primary sponsor we will be choosing is Gulfport-Biloxi, MS Metro Area, which had the least burden changes, and actually decreasing it, with a negative percentage of -15.9170722%.\nOur cosponsor would preferably be a city which we can work from scratch. There are cities whose housing development does not seem to increase, while the rent keeps increasing, leaving locals with a heavy burden. In Texarkana, TX-AR Metro Area, the rent burden keeps increasing with a 22.3293759% while population appears to be decreasing by -5825 in 5 years.\n\n\nRecommendation\nOur polices should be implemented into either city as they will make one city continue to thrive, while the other will begin to flourish. Locals deserve to stay in their home, especially as natives, yet unfortunately some are forcefully relocated elsewhere due to high rent prices. That is why we need the help of not only the Congress, but also of the people. We encourage all local municipalities to allow us to cooperate with them to transform their home into a greater place.\n\nThe metrics we use are based on Rent Burden Index, a Housing Growth Index, and Household Trends, to name a few. These allow us to see any changes from 2009-2023.\n\n\n\nAllow us to help you live and thrive!"
  },
  {
    "objectID": "mp02.html#task-1",
    "href": "mp02.html#task-1",
    "title": "Making Backyards Affordable for All",
    "section": "Task 1",
    "text": "Task 1"
  }
]